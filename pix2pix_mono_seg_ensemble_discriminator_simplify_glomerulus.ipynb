{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "{'img_shape': [512, 512], 'input_channels': 3, 'output_channels': 1}\n"
     ]
    }
   ],
   "source": [
    "# TBD 1 : logger 추가\n",
    "# TBD 2: flask github 참고, method, class, 파일의 맨 윗단 마다 pydoc 형식으로 달기\n",
    "# TBD 3: 축약어를 자제할것 (특히 변수)\n",
    "\n",
    "# -------------------------\n",
    "#   done\n",
    "# -------------------------\n",
    "\n",
    "# 0. add data-setter, receiver system use python queue.Queue() class\n",
    "# this will resolve i/o bottleneck\n",
    "# 3. make iterable\n",
    "\n",
    "# -------------------------\n",
    "#   In Progress\n",
    "# -------------------------\n",
    "\n",
    "# 1. add logger\n",
    "# 2. make image drawer overlay mask on image\n",
    "\n",
    "# -------------------------\n",
    "#   To be Done\n",
    "# -------------------------\n",
    "\n",
    "# 4. make verbose turn on and off\n",
    "# 5. write pydoc\n",
    "\n",
    "# python basic Module\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import progressbar\n",
    "from datetime import datetime\n",
    "from shutil import copy\n",
    "from pickle import dump, load\n",
    "\n",
    "# math, image, plot Module\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt  # TBD\n",
    "\n",
    "# tensorflow Module\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as keras_backend\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "# keras segmentaion third-party Moudle\n",
    "import segmentation_models as sm\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# custom Module\n",
    "from gan_module.data_loader.medical_segmentation_data_loader import DataLoader\n",
    "from gan_module.data_loader.manage_batch import BatchQueueManager\n",
    "\n",
    "from gan_module.model.build_model import build_generator_non_unet as build_generator\n",
    "from gan_module.model.build_model import build_discriminator as build_discriminator\n",
    "from gan_module.util.custom_loss import weighted_region_loss, dice_score, combined_loss, f1_loss\n",
    "# from gan_module.util.custom_gradient import SGD_AGC\n",
    "from gan_module.util.manage_learning_rate import learning_rate_scheduler\n",
    "from gan_module.util.draw_images import ImageDrawer\n",
    "from gan_module.util.logger import TrainLogger\n",
    "from gan_module.config import CONFIG\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "class Pix2PixSegmentation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_power=32,\n",
    "        discriminator_power=32,\n",
    "        generator_depth = None,\n",
    "        discriminator_depth = None,\n",
    "        generator_learning_rate=1e-4,\n",
    "        discriminator_learning_rate=1e-4,\n",
    "        temp_weights_path=\".\",\n",
    "        on_memory=True,\n",
    "        code_test=False\n",
    "    ):\n",
    "        # Input shape\n",
    "        img_shape = CONFIG[\"img_shape\"]\n",
    "        input_channels = CONFIG[\"input_channels\"]\n",
    "        output_channels = CONFIG[\"output_channels\"]\n",
    "\n",
    "        self.input_img_shape = (*img_shape, input_channels)\n",
    "        self.output_img_shape = (*img_shape, output_channels)\n",
    "        # set parameter\n",
    "        self.start_epoch = None\n",
    "        self.on_memory = on_memory\n",
    "        self.history = {\"generator_loss\": [],\n",
    "                        \"f1_loss_train\": [], \"f1_score_train\": [],\n",
    "                        \"f1_loss_valid\": [], \"f1_score_valid\": []}\n",
    "        self.temp_weights_path = temp_weights_path\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = \"glomerulus_0.65_512_not_filped_original\"\n",
    "        self.data_loader = DataLoader(\n",
    "            dataset_name=self.dataset_name,\n",
    "            config_dict=CONFIG,\n",
    "            on_memory=self.on_memory, \n",
    "            code_test=code_test\n",
    "        )\n",
    "        \n",
    "        self.train_logger = TrainLogger()\n",
    "        \n",
    "        self.loaded_data_index = {\n",
    "            \"train\": np.arange(self.data_loader.data_length[\"train\"]),\n",
    "            \"valid\": np.arange(self.data_loader.data_length[\"valid\"])\n",
    "        }\n",
    "        \n",
    "        # Configure Image Drawer\n",
    "        self.image_drawer = ImageDrawer(\n",
    "            dataset_name=self.dataset_name, data_loader=self.data_loader\n",
    "        )\n",
    "        self.discriminator_loss_ratio = keras_backend.variable(0.1)\n",
    "        self.f1_loss_ratio = keras_backend.variable(100)\n",
    "        self.discriminator_losses = np.array(\n",
    "            [1 for _ in range(self.data_loader.data_length[\"train\"])], dtype=np.float32)\n",
    "        self.discriminator_acc_previous = 0.5\n",
    "        self.discriminator_acces = np.array(\n",
    "            [0.5 for _ in range(self.data_loader.data_length[\"train\"])])\n",
    "        self.discriminator_acces_previous = self.discriminator_acces.copy()\n",
    "        self.generator_losses = np.array(\n",
    "            [1 for _ in range(self.data_loader.data_length[\"train\"])], dtype=np.float32)\n",
    "        self.generator_losses_previous = self.generator_losses.copy()\n",
    "        self.generator_f1_losses = np.array(\n",
    "            [1 for _ in range(self.data_loader.data_length[\"train\"])], dtype=np.float32)\n",
    "        self.generator_loss_min = 10000\n",
    "        self.generator_loss_previous = 1000\n",
    "        self.generator_loss_max_previous = 1000\n",
    "        self.total_f1_loss_min = 2\n",
    "        self.weight_save_stack = False\n",
    "        self.training_end_stack = 0\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        self.disc_patch = (img_shape[0] // (2 ** discriminator_depth), img_shape[1] // (2 ** discriminator_depth), 1)\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "        self.patience_count = 0\n",
    "        \n",
    "        generator_optimizer = Nadam(self.generator_learning_rate)\n",
    "        discriminator_optimizer = Nadam(self.discriminator_learning_rate)\n",
    "#         generator_optimizer = SGD_AGC(lr=self.generator_learning_rate, momentum=0.9)\n",
    "#         discriminator_optimizer = SGD_AGC(lr=self.discriminator_learning_rate, momentum=0.9)        \n",
    "        # Build the generator\n",
    "        self.generator = build_generator(\n",
    "            input_img_shape=self.input_img_shape,\n",
    "            output_channels=output_channels,\n",
    "            generator_power=generator_power,\n",
    "            depth=generator_depth,\n",
    "        )\n",
    "        self.generator.compile(\n",
    "            loss=weighted_region_loss,\n",
    "            optimizer=generator_optimizer,\n",
    "            metrics=[dice_score],\n",
    "        )\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator(\n",
    "            input_img_shape=self.input_img_shape,\n",
    "            output_img_shape=self.output_img_shape,\n",
    "            discriminator_power=discriminator_power,\n",
    "            depth=discriminator_depth,\n",
    "        )\n",
    "        # 'mse' or tf.keras.losses.Huber() tf.keras.losses.LogCosh()\n",
    "        self.discriminator.compile(\n",
    "            loss=sm.losses.BinaryFocalLoss(alpha=0.25, gamma=4),\n",
    "            optimizer=discriminator_optimizer,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # -------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        # -------------------------\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        original_img = Input(shape=self.input_img_shape)\n",
    "        # generate image from original_img for target masked_img\n",
    "        model_masked_img = self.generator(original_img)\n",
    "        \n",
    "    \n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        model_validity = self.discriminator([original_img, model_masked_img])\n",
    "        # give score by\n",
    "        # 1. how generator trick discriminator\n",
    "        # 2. how generator's image same as real photo in pixel\n",
    "        # 3. if you want change loss, see doc https://keras.io/api/losses/\n",
    "        # 4. 'mse', 'mae', tf.keras.losses.LogCosh(),  tf.keras.losses.Huber()\n",
    "        self.combined = Model(\n",
    "            inputs=original_img,\n",
    "            outputs=[model_validity, model_masked_img],\n",
    "        )\n",
    "        \n",
    "        self.combined.compile(\n",
    "            loss=[\n",
    "#                 tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),\n",
    "                sm.losses.BinaryFocalLoss(alpha=0.25, gamma=4),\n",
    "                weighted_region_loss\n",
    "            ],\n",
    "            loss_weights=[0.1, 100],\n",
    "            optimizer=generator_optimizer,\n",
    "        )\n",
    "\n",
    "    def train(self, epochs, batch_size=1, epoch_shuffle_term=10):\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        self.training_end_stack = 0\n",
    "        self.batch_size = batch_size\n",
    "        valid_patch = np.ones(\n",
    "            (self.batch_size, *self.disc_patch), dtype=np.float32)\n",
    "        fake_patch = np.zeros(\n",
    "            (self.batch_size, *self.disc_patch), dtype=np.float32)\n",
    "        # TBD : move batch_queue_manager to __init__\n",
    "        self.batch_queue_manager = BatchQueueManager(self, batch_size, self.on_memory)\n",
    "        \n",
    "        if self.start_epoch is None:\n",
    "            self.start_epoch = 0\n",
    "        for epoch in range(self.start_epoch, epochs):\n",
    "            batch_i = 0\n",
    "\n",
    "            generator_loss_max_in_epoch = 0\n",
    "            generator_loss_min_in_epoch = 1000\n",
    "            generator_discriminator_losses = np.array(\n",
    "            [1 for _ in range(self.data_loader.data_length[\"train\"])], dtype=np.float32)\n",
    "            # shffle data maybe\n",
    "            if epoch % epoch_shuffle_term == 0:\n",
    "                self.data_loader.shuffle_train_imgs()\n",
    "            \n",
    "            if self.discriminator_acc_previous < 0.95:\n",
    "                discriminator_learning = True\n",
    "            else:\n",
    "                discriminator_learning = False\n",
    "            generator_1_10_quantile = np.quantile(self.generator_losses, 0.1)\n",
    "            \n",
    "            \n",
    "            generator_current_learning_rate = learning_rate_scheduler(\n",
    "                self.generator_learning_rate,\n",
    "                epoch+self.patience_count,\n",
    "                warm_up=True\n",
    "            )\n",
    "            discriminator_current_learning_rate = learning_rate_scheduler(\n",
    "                self.discriminator_learning_rate,\n",
    "                epoch+self.patience_count,\n",
    "                warm_up=True\n",
    "            ) * (1 - self.discriminator_acc_previous)\n",
    "            keras_backend.set_value(\n",
    "                self.discriminator.optimizer.learning_rate,\n",
    "                discriminator_current_learning_rate,\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.discriminator.optimizer.learning_rate,\n",
    "                discriminator_current_learning_rate,\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.discriminator_loss_ratio,\n",
    "                keras_backend.variable(0.01) + 0.75 * self.discriminator_acc_previous,\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.f1_loss_ratio,\n",
    "                keras_backend.variable(100) - 0.75  * self.discriminator_acc_previous,\n",
    "            )\n",
    "            \n",
    "            bar = progressbar.ProgressBar(\n",
    "                maxval=self.data_loader.data_length[\"train\"]).start()\n",
    "            \n",
    "            while batch_i + self.batch_size < self.data_loader.data_length[\"train\"] + self.batch_size:\n",
    "\n",
    "                batch_index = self.loaded_data_index[\"train\"][batch_i: batch_i +\n",
    "                                                              self.batch_size]\n",
    "\n",
    "                original_img, masked_img = self.batch_queue_manager.get_batch(\n",
    "                    data_mode=\"train\")\n",
    "                model_masked_img = self.generator.predict_on_batch(\n",
    "                    original_img)\n",
    "                \n",
    "                valid_patch = np.ones(\n",
    "                    (len(model_masked_img), *self.disc_patch), dtype=np.float32)\n",
    "                fake_patch = np.zeros(\n",
    "                    (len(model_masked_img), *self.disc_patch), dtype=np.float32)\n",
    "                \n",
    "                self.original_img = original_img\n",
    "                self.masked_img = masked_img\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                # Train Discriminator for valid image if it failed to detect fake image\n",
    "                if discriminator_learning and self.discriminator_acc_previous < np.random.rand():\n",
    "                    discriminator_loss = self.discriminator.train_on_batch([original_img, masked_img], valid_patch)\n",
    "                else:\n",
    "                    discriminator_loss = self.discriminator.test_on_batch([original_img, masked_img], valid_patch)\n",
    "                    \n",
    "                batch_discriminator_acc_previous = np.mean(\n",
    "                    self.discriminator_acces_previous[batch_index])\n",
    "                self.discriminator.trainable = False\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "                \n",
    "                if np.mean(self.generator_losses[batch_index]) >= generator_1_10_quantile:\n",
    "                    generator_loss = self.combined.train_on_batch(\n",
    "                        original_img,\n",
    "                        [valid_patch, masked_img]\n",
    "                    )\n",
    "                else:\n",
    "                    generator_loss = self.combined.test_on_batch(\n",
    "                        original_img,\n",
    "                        [valid_patch, masked_img]\n",
    "                    )                    \n",
    "                # train discriminator for fake image if it failed to detect fake image\n",
    "                self.discriminator.trainable = True\n",
    "                if (batch_discriminator_acc_previous <= 0.5 or epoch == 0) and discriminator_learning:\n",
    "                    discriminator_loss += self.discriminator.train_on_batch(\n",
    "                        [original_img,model_masked_img], fake_patch)\n",
    "                else:\n",
    "                    discriminator_loss += self.discriminator.test_on_batch(\n",
    "                        [original_img,model_masked_img], fake_patch)\n",
    "\n",
    "                self.discriminator_losses[batch_index] = discriminator_loss[0]\n",
    "                self.discriminator_acces[batch_index] = discriminator_loss[1]\n",
    "                self.generator_losses[batch_index] = generator_loss[0]\n",
    "                self.generator_f1_losses[batch_index] = generator_loss[2]\n",
    "                generator_discriminator_losses = generator_loss[1]\n",
    "                # plot progress\n",
    "                bar.update(batch_i)\n",
    "                \n",
    "                # 한 배치 끝\n",
    "                batch_i += self.batch_size\n",
    "            \n",
    "            \n",
    "            # training batch 사이클 끝\n",
    "            \n",
    "            self.train_logger.write_log(\n",
    "                f\"{epoch}/{epochs} ({epoch+self.patience_count})\",\n",
    "                np.mean(self.discriminator_acces),\n",
    "                np.mean(self.generator_losses),\n",
    "                np.max(self.generator_losses),\n",
    "                np.min(self.generator_losses),\n",
    "                f\"{self.generator_loss_min - np.mean(self.generator_losses)}({np.mean(self.generator_losses) / self.generator_loss_min})\",\n",
    "                self.generator_loss_min,\n",
    "                generator_current_learning_rate,\n",
    "                datetime.now() - start_time\n",
    "            )\n",
    "            self.image_drawer.sample_images(\n",
    "                self.generator, epoch)\n",
    "            \n",
    "            if np.mean(self.generator_losses) / self.generator_loss_min < 1.1:\n",
    "                #if self.generator_loss_min > np.mean(self.generator_losses):\n",
    "                valid_f1_loss_list = []\n",
    "                for index in range(0, self.data_loader.data_length[\"valid\"], self.batch_size):\n",
    "\n",
    "                    valid_source_img, valid_masked_img = self.batch_queue_manager.get_batch(\n",
    "                        data_mode=\"valid\")\n",
    "\n",
    "                    valid_model_masked_img = self.generator.predict_on_batch(\n",
    "                        valid_source_img)\n",
    "                    valid_f1_loss = dice_score(\n",
    "                        valid_masked_img, valid_model_masked_img)\n",
    "                    valid_f1_loss_list.append(valid_f1_loss)\n",
    "                # compute valid_f1_loss end    \n",
    "                total_f1_loss = np.mean(self.generator_f1_losses) + np.mean(valid_f1_loss_list)\n",
    "\n",
    "                print(f\"discriminator_loss : {np.mean(self.discriminator_losses)}\")\n",
    "                print(f\"generator_discriminator_loss : {np.mean(generator_discriminator_losses)}\")\n",
    "                print(f\"train_f1_loss : {np.mean(self.generator_f1_losses)}\")\n",
    "                print(\n",
    "                    f\"valid_f1_loss : {np.mean(valid_f1_loss_list)}\")\n",
    "                print(f\"current/min total_f1_loss = {total_f1_loss} / {self.total_f1_loss_min}\")\n",
    "                if self.generator_loss_min > np.mean(self.generator_losses):\n",
    "                    self.generator_loss_min = np.mean(self.generator_losses)\n",
    "                    self.generator_loss_max_min = generator_loss_max_in_epoch\n",
    "                    self.generator_loss_min_min = generator_loss_min_in_epoch\n",
    "                    self.save_study_info()\n",
    "                    self.weight_save_stack = True\n",
    "                    print(\"save weights\")                \n",
    "                if self.total_f1_loss_min > total_f1_loss: \n",
    "                    self.total_f1_loss_min = total_f1_loss\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"loss decrease.\")\n",
    "                self.patience_count += 1\n",
    "                self.load_best_weights()\n",
    "\n",
    "            # previous generator_loss 갱신\n",
    "            self.generator_loss_previous = np.mean(self.generator_losses)\n",
    "            self.generator_loss_max_previous = np.max(self.generator_losses)\n",
    "\n",
    "            if epoch >= 10 and self.weight_save_stack:\n",
    "                copy(\n",
    "                    \"generator.h5\",\n",
    "                    \"./generator_weights/generator_\"\n",
    "                    + str(round(self.generator_loss_min, 5))\n",
    "                    + \"_\"\n",
    "                    + str(round(self.generator_loss_max_min, 5))\n",
    "                    + \".h5\",\n",
    "                )\n",
    "                self.weight_save_stack = False\n",
    "\n",
    "            self.discriminator_acc_previous = np.mean(self.discriminator_acces)\n",
    "            self.discriminator_acces_previous = self.discriminator_acces.copy()\n",
    "            self.generator_losses_previous = self.generator_losses.copy()\n",
    "            # TBD: add epoch bigger than history length\n",
    "            self.history[\"generator_loss\"].append(\n",
    "                np.mean(self.generator_losses))\n",
    "            self.history[\"f1_loss_train\"].append(\n",
    "                np.mean(self.generator_f1_losses))\n",
    "            self.history[\"f1_loss_valid\"].append(\n",
    "                np.mean(valid_f1_loss_list))\n",
    "\n",
    "    def get_info_folderPath(self):\n",
    "        return (\n",
    "            str(round(self.generator_loss_min, 5))\n",
    "            + \"_\"\n",
    "            + str(round(self.generator_loss_max_min, 5))\n",
    "        )\n",
    "\n",
    "    def save_study_info(self, path=None):\n",
    "\n",
    "        if path is None:\n",
    "            path = self.temp_weights_path\n",
    "\n",
    "        generator_weigth_path = os.path.join(path, \"generator.h5\")\n",
    "        discriminator_weigth_path = os.path.join(path, \"discriminator.h5\")\n",
    "        combined_weigth_path = os.path.join(path, \"combined.h5\")\n",
    "\n",
    "        self.generator.save_weights(generator_weigth_path)\n",
    "        self.discriminator.save_weights(discriminator_weigth_path)\n",
    "        self.combined.save_weights(combined_weigth_path)\n",
    "\n",
    "        study_info = {}\n",
    "        study_info[\"start_epoch\"] = self.start_epoch\n",
    "        study_info[\"train_loaded_data_index\"] = self.loaded_data_index[\"train\"]\n",
    "        study_info[\"generator_loss_min\"] = self.generator_loss_min\n",
    "        study_info[\"generator_loss_max_min\"] = self.generator_loss_max_min\n",
    "        study_info[\"generator_loss_min_min\"] = self.generator_loss_min_min\n",
    "        study_info[\"generator_losses_previous\"] = self.generator_losses_previous\n",
    "        study_info[\"discriminator_acces\"] = self.discriminator_acces\n",
    "        study_info[\"history\"] = self.history\n",
    "        file = open(path + \"/study_info.pkl\", \"wb\")\n",
    "        dump(study_info, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_study_info(self):\n",
    "\n",
    "        self.generator.load_weights(\"generator.h5\")\n",
    "        self.discriminator.load_weights(\"discriminator.h5\")\n",
    "#         self.combined.load_weights(\"combined.h5\")\n",
    "\n",
    "        if os.path.isfile(\"study_info.pkl\"):\n",
    "            file = open(\"study_info.pkl\", \"rb\")\n",
    "            study_info = load(file)\n",
    "            file.close()\n",
    "            self.start_epoch = study_info[\"start_epoch\"]\n",
    "            self.loaded_data_index[\"train\"] = study_info[\"train_loaded_data_index\"]\n",
    "            self.generator_loss_min = study_info[\"generator_loss_min\"]\n",
    "            self.generator_loss_max_min = study_info[\"generator_loss_max_min\"]\n",
    "            self.generator_loss_min_min = study_info[\"generator_loss_min_min\"]\n",
    "            self.generator_losses_previous = study_info[\"generator_losses_previous\"]\n",
    "            self.discriminator_acces = study_info[\"discriminator_acces\"]\n",
    "            self.history = study_info[\"history\"]\n",
    "        else:\n",
    "            print(\"No info pkl file!\")\n",
    "\n",
    "    def load_best_weights(self):\n",
    "        self.generator.load_weights(self.temp_weights_path + \"/generator.h5\")\n",
    "        self.discriminator.load_weights(\n",
    "            self.temp_weights_path + \"/discriminator.h5\")\n",
    "        self.combined.load_weights(self.temp_weights_path + \"/combined.h5\")\n",
    "\n",
    "    def run_pretraining(self, epochs):\n",
    "        if self.on_memory:\n",
    "            self.generator.fit(\n",
    "                x=self.data_loader.loaded_data_object[\"train\"][\"input\"],\n",
    "                y=self.data_loader.loaded_data_object[\"train\"][\"output\"],\n",
    "                validation_data=list(self.data_loader.loaded_data_object[\"valid\"].values()),\n",
    "                batch_size=self.batch_size, epochs=epochs\n",
    "            )\n",
    "        else:\n",
    "            self.generator.fit_generator(\n",
    "                x=self.data_loader.loaded_data_object[\"train\"][\"input\"],\n",
    "                y=self.data_loader.loaded_data_object[\"train\"][\"output\"],\n",
    "                validation_data=list(self.data_loader.loaded_data_object[\"valid\"].values()),\n",
    "                batch_size=self.batch_size, epochs=epochs\n",
    "            )\n",
    "        self.generator.save_weights(\"pretrained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_lr = 1e-3\n",
    "discriminator_lr = 1e-3\n",
    "batch_size = 4\n",
    "\n",
    "g_lr = generator_lr * batch_size\n",
    "d_lr = discriminator_lr * batch_size\n",
    "gan = Pix2PixSegmentation(generator_power=8, discriminator_power=8, \n",
    "                          generator_depth = 3, discriminator_depth = 3,\n",
    "                          generator_learning_rate=g_lr, discriminator_learning_rate=d_lr,\n",
    "                          on_memory=False, code_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:20:44 ETA:   0:00:00{\n",
      "2021-05-23 09:57:49,311 - train - INFO - \n",
      "Epoch : 0/200 (0)\n",
      "Discriminator_acces : 0.5533265458776596\n",
      "Mean generator loss : 69.21029663085938\n",
      "Max generator loss : 157.54681396484375\n",
      "Min generator loss : 32.42213439941406\n",
      "Generator loss decrease : 9930.78970336914(0.006921029663085937)\n",
      "Current lowest generator loss : 10000\n",
      "Current Learning_rate : 0.0002\n",
      "Elapsed_time : 0:20:44.132739\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.8816938996315002\n",
      "generator_discriminator_loss : 0.010948128998279572\n",
      "train_f1_loss : 0.6620980501174927\n",
      "valid_f1_loss : 0.3546465337276459\n",
      "current/min total_f1_loss = 1.016744613647461 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:17:17 ETA:   0:00:00{\n",
      "2021-05-23 10:15:23,724 - train - INFO - \n",
      "Epoch : 1/200 (1)\n",
      "Discriminator_acces : 0.49375696463960184\n",
      "Mean generator loss : 28.009563446044922\n",
      "Max generator loss : 104.8630142211914\n",
      "Min generator loss : 13.092679023742676\n",
      "Generator loss decrease : 41.20073318481445(0.40470224618911743)\n",
      "Current lowest generator loss : 69.21029663085938\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 0:38:18.545359\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.15179386734962463\n",
      "generator_discriminator_loss : 0.011268626898527145\n",
      "train_f1_loss : 0.27101269364356995\n",
      "valid_f1_loss : 0.6469917297363281\n",
      "current/min total_f1_loss = 0.9180043935775757 / 1.016744613647461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:19:18 ETA:   0:00:00{\n",
      "2021-05-23 10:34:57,921 - train - INFO - \n",
      "Epoch : 2/200 (2)\n",
      "Discriminator_acces : 0.3529657664512756\n",
      "Mean generator loss : 19.221960067749023\n",
      "Max generator loss : 68.08334350585938\n",
      "Min generator loss : 6.9575629234313965\n",
      "Generator loss decrease : 8.787603378295898(0.686264157295227)\n",
      "Current lowest generator loss : 28.009563446044922\n",
      "Current Learning_rate : 0.0006\n",
      "Elapsed_time : 0:57:52.741436\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.0848349928855896\n",
      "generator_discriminator_loss : 0.011807028204202652\n",
      "train_f1_loss : 0.18207761645317078\n",
      "valid_f1_loss : 0.5213815569877625\n",
      "current/min total_f1_loss = 0.7034591436386108 / 0.9180043935775757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:07 ETA:   0:00:00{\n",
      "2021-05-23 10:56:27,069 - train - INFO - \n",
      "Epoch : 3/200 (3)\n",
      "Discriminator_acces : 0.09674741748541107\n",
      "Mean generator loss : 16.095069885253906\n",
      "Max generator loss : 189.1872100830078\n",
      "Min generator loss : 3.7259206771850586\n",
      "Generator loss decrease : 3.126890182495117(0.8373271822929382)\n",
      "Current lowest generator loss : 19.221960067749023\n",
      "Current Learning_rate : 0.0008\n",
      "Elapsed_time : 1:19:21.891035\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.02525668777525425\n",
      "generator_discriminator_loss : 0.014166601002216339\n",
      "train_f1_loss : 0.1514119803905487\n",
      "valid_f1_loss : 0.5231001377105713\n",
      "current/min total_f1_loss = 0.6745121479034424 / 0.7034591436386108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:20:26 ETA:   0:00:00{\n",
      "2021-05-23 11:17:09,230 - train - INFO - \n",
      "Epoch : 4/200 (4)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 15.374262809753418\n",
      "Max generator loss : 694.0433959960938\n",
      "Min generator loss : 4.392180919647217\n",
      "Generator loss decrease : 0.7208070755004883(0.9552156329154968)\n",
      "Current lowest generator loss : 16.095069885253906\n",
      "Current Learning_rate : 0.001\n",
      "Elapsed_time : 1:40:04.051350\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.015496288426220417\n",
      "generator_discriminator_loss : 0.017307713627815247\n",
      "train_f1_loss : 0.13665840029716492\n",
      "valid_f1_loss : 0.11646916717290878\n",
      "current/min total_f1_loss = 0.2531275749206543 / 0.6745121479034424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:01 ETA:   0:00:00{\n",
      "2021-05-23 11:38:27,308 - train - INFO - \n",
      "Epoch : 5/200 (5)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 12.690038681030273\n",
      "Max generator loss : 148.67645263671875\n",
      "Min generator loss : 3.089963912963867\n",
      "Generator loss decrease : 2.6842241287231445(0.8254079222679138)\n",
      "Current lowest generator loss : 15.374262809753418\n",
      "Current Learning_rate : 0.0012\n",
      "Elapsed_time : 2:01:22.129524\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.018657371401786804\n",
      "generator_discriminator_loss : 0.019520027562975883\n",
      "train_f1_loss : 0.11294656991958618\n",
      "valid_f1_loss : 0.5246710777282715\n",
      "current/min total_f1_loss = 0.6376176476478577 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:22:37 ETA:   0:00:00{\n",
      "2021-05-23 12:01:20,696 - train - INFO - \n",
      "Epoch : 6/200 (6)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 12.88043212890625\n",
      "Max generator loss : 209.487548828125\n",
      "Min generator loss : 2.504220962524414\n",
      "Generator loss decrease : -0.19039344787597656(1.0150033235549927)\n",
      "Current lowest generator loss : 12.690038681030273\n",
      "Current Learning_rate : 0.0014\n",
      "Elapsed_time : 2:24:15.517174\n",
      "}\n",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.01985093764960766\n",
      "generator_discriminator_loss : 0.020018260926008224\n",
      "train_f1_loss : 0.11099027842283249\n",
      "valid_f1_loss : 0.5230262875556946\n",
      "current/min total_f1_loss = 0.6340165734291077 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:16 ETA:   0:00:00{\n",
      "2021-05-23 12:22:58,156 - train - INFO - \n",
      "Epoch : 7/200 (7)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 13.231287956237793\n",
      "Max generator loss : 796.4058227539062\n",
      "Min generator loss : 1.6052820682525635\n",
      "Generator loss decrease : -0.5412492752075195(1.0426515340805054)\n",
      "Current lowest generator loss : 12.690038681030273\n",
      "Current Learning_rate : 0.0016\n",
      "Elapsed_time : 2:45:52.977333\n",
      "}\n",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.020074184983968735\n",
      "generator_discriminator_loss : 0.020084913820028305\n",
      "train_f1_loss : 0.11628202348947525\n",
      "valid_f1_loss : 0.6652283072471619\n",
      "current/min total_f1_loss = 0.7815103530883789 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:16 ETA:   0:00:00{\n",
      "2021-05-23 12:44:31,144 - train - INFO - \n",
      "Epoch : 8/200 (8)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 12.282870292663574\n",
      "Max generator loss : 758.059814453125\n",
      "Min generator loss : 1.5115694999694824\n",
      "Generator loss decrease : 0.4071683883666992(0.967914342880249)\n",
      "Current lowest generator loss : 12.690038681030273\n",
      "Current Learning_rate : 0.0018000000000000002\n",
      "Elapsed_time : 3:07:25.965499\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.020118659362196922\n",
      "generator_discriminator_loss : 0.020091373473405838\n",
      "train_f1_loss : 0.10674536973237991\n",
      "valid_f1_loss : 0.5605238676071167\n",
      "current/min total_f1_loss = 0.667269229888916 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:13 ETA:   0:00:00{\n",
      "2021-05-23 13:06:01,131 - train - INFO - \n",
      "Epoch : 9/200 (9)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 12.20828628540039\n",
      "Max generator loss : 1065.598876953125\n",
      "Min generator loss : 1.2386300563812256\n",
      "Generator loss decrease : 0.0745840072631836(0.9939277768135071)\n",
      "Current lowest generator loss : 12.282870292663574\n",
      "Current Learning_rate : 0.002\n",
      "Elapsed_time : 3:28:55.952611\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.02012895606458187\n",
      "generator_discriminator_loss : 0.02016810141503811\n",
      "train_f1_loss : 0.10665664821863174\n",
      "valid_f1_loss : 0.5213815569877625\n",
      "current/min total_f1_loss = 0.628038227558136 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:14 ETA:   0:00:00{\n",
      "2021-05-23 13:27:31,303 - train - INFO - \n",
      "Epoch : 10/200 (10)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 8.707088470458984\n",
      "Max generator loss : 44.35485076904297\n",
      "Min generator loss : 1.3509299755096436\n",
      "Generator loss decrease : 3.5011978149414062(0.7132113575935364)\n",
      "Current lowest generator loss : 12.20828628540039\n",
      "Current Learning_rate : 0.0022\n",
      "Elapsed_time : 3:50:26.124410\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.02013368345797062\n",
      "generator_discriminator_loss : 0.020070770755410194\n",
      "train_f1_loss : 0.07386530190706253\n",
      "valid_f1_loss : 0.5213815569877625\n",
      "current/min total_f1_loss = 0.5952468514442444 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:24:02 ETA:   0:00:00{\n",
      "2021-05-23 13:51:49,845 - train - INFO - \n",
      "Epoch : 11/200 (11)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.273225784301758\n",
      "Max generator loss : 319.9456481933594\n",
      "Min generator loss : 1.173499345779419\n",
      "Generator loss decrease : -1.5661373138427734(1.1798691749572754)\n",
      "Current lowest generator loss : 8.707088470458984\n",
      "Current Learning_rate : 0.0024\n",
      "Elapsed_time : 4:14:44.666555\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:25:23 ETA:   0:00:00{\n",
      "2021-05-23 14:17:14,746 - train - INFO - \n",
      "Epoch : 12/200 (13)\n",
      "Discriminator_acces : 0.0003164094776389176\n",
      "Mean generator loss : 7.525634765625\n",
      "Max generator loss : 48.02891159057617\n",
      "Min generator loss : 1.1562256813049316\n",
      "Generator loss decrease : 1.1814537048339844(0.8643112778663635)\n",
      "Current lowest generator loss : 8.707088470458984\n",
      "Current Learning_rate : 0.0028\n",
      "Elapsed_time : 4:40:09.567219\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.02688809111714363\n",
      "generator_discriminator_loss : 0.02013746276497841\n",
      "train_f1_loss : 0.06088100001215935\n",
      "valid_f1_loss : 0.7270432710647583\n",
      "current/min total_f1_loss = 0.7879242897033691 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:30:12 ETA:   0:00:00{\n",
      "2021-05-23 14:47:50,231 - train - INFO - \n",
      "Epoch : 13/200 (14)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 11.960439682006836\n",
      "Max generator loss : 634.6890258789062\n",
      "Min generator loss : 1.0178649425506592\n",
      "Generator loss decrease : -4.434804916381836(1.589293122291565)\n",
      "Current lowest generator loss : 7.525634765625\n",
      "Current Learning_rate : 0.003\n",
      "Elapsed_time : 5:10:45.052413\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:30:36 ETA:   0:00:00{\n",
      "2021-05-23 15:18:28,498 - train - INFO - \n",
      "Epoch : 14/200 (16)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 7.242619037628174\n",
      "Max generator loss : 28.86436653137207\n",
      "Min generator loss : 0.9633047580718994\n",
      "Generator loss decrease : 0.28301572799682617(0.9623931050300598)\n",
      "Current lowest generator loss : 7.525634765625\n",
      "Current Learning_rate : 0.0034\n",
      "Elapsed_time : 5:41:23.319267\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.020147092640399933\n",
      "generator_discriminator_loss : 0.0200914666056633\n",
      "train_f1_loss : 0.061414919793605804\n",
      "valid_f1_loss : 0.680955708026886\n",
      "current/min total_f1_loss = 0.74237060546875 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:29:31 ETA:   0:00:00{\n",
      "2021-05-23 15:48:22,417 - train - INFO - \n",
      "Epoch : 15/200 (17)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 17.56414222717285\n",
      "Max generator loss : 2543.5068359375\n",
      "Min generator loss : 0.9591549634933472\n",
      "Generator loss decrease : -10.321523666381836(2.4251091480255127)\n",
      "Current lowest generator loss : 7.242619037628174\n",
      "Current Learning_rate : 0.0036000000000000003\n",
      "Elapsed_time : 6:11:17.239008\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:28:18 ETA:   0:00:00{\n",
      "2021-05-23 16:16:42,457 - train - INFO - \n",
      "Epoch : 16/200 (19)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 6.2098236083984375\n",
      "Max generator loss : 145.08584594726562\n",
      "Min generator loss : 0.7874513268470764\n",
      "Generator loss decrease : 1.0327954292297363(0.8574002981185913)\n",
      "Current lowest generator loss : 7.242619037628174\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 6:39:37.278461\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.02020816132426262\n",
      "generator_discriminator_loss : 0.020108411088585854\n",
      "train_f1_loss : 0.05315054953098297\n",
      "valid_f1_loss : 0.7771904468536377\n",
      "current/min total_f1_loss = 0.8303409814834595 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:21 ETA:   0:00:00{\n",
      "2021-05-23 16:38:21,162 - train - INFO - \n",
      "Epoch : 17/200 (20)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 5.838086128234863\n",
      "Max generator loss : 125.19440460205078\n",
      "Min generator loss : 0.8176804184913635\n",
      "Generator loss decrease : 0.3717374801635742(0.9401372075080872)\n",
      "Current lowest generator loss : 6.2098236083984375\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 7:01:15.983997\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.020199881866574287\n",
      "generator_discriminator_loss : 0.020111041143536568\n",
      "train_f1_loss : 0.049415018409490585\n",
      "valid_f1_loss : 0.6191221475601196\n",
      "current/min total_f1_loss = 0.6685371398925781 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:22 ETA:   0:00:00{\n",
      "2021-05-23 17:00:00,176 - train - INFO - \n",
      "Epoch : 18/200 (21)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 5.874327659606934\n",
      "Max generator loss : 183.0410614013672\n",
      "Min generator loss : 0.7856991291046143\n",
      "Generator loss decrease : -0.03624153137207031(1.006207823753357)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 7:22:54.997788\n",
      "}\n",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.020200099796056747\n",
      "generator_discriminator_loss : 0.02015145868062973\n",
      "train_f1_loss : 0.050091702491045\n",
      "valid_f1_loss : 0.8064138293266296\n",
      "current/min total_f1_loss = 0.8565055131912231 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:19 ETA:   0:00:00{\n",
      "2021-05-23 17:21:35,742 - train - INFO - \n",
      "Epoch : 19/200 (22)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 7.601513385772705\n",
      "Max generator loss : 780.6641845703125\n",
      "Min generator loss : 0.8175713419914246\n",
      "Generator loss decrease : -1.7634272575378418(1.3020557165145874)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 7:44:30.563796\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:19 ETA:   0:00:00{\n",
      "2021-05-23 17:42:56,185 - train - INFO - \n",
      "Epoch : 20/200 (24)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.810599327087402\n",
      "Max generator loss : 722.5655517578125\n",
      "Min generator loss : 0.8561497926712036\n",
      "Generator loss decrease : -4.972513198852539(1.8517369031906128)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 8:05:51.006347\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:23 ETA:   0:00:00{\n",
      "2021-05-23 18:04:21,146 - train - INFO - \n",
      "Epoch : 21/200 (26)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.385814666748047\n",
      "Max generator loss : 413.0534973144531\n",
      "Min generator loss : 0.7462989687919617\n",
      "Generator loss decrease : -4.547728538513184(1.7789759635925293)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 8:27:15.967224\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:26 ETA:   0:00:00{\n",
      "2021-05-23 18:25:48,832 - train - INFO - \n",
      "Epoch : 22/200 (28)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 13.149188041687012\n",
      "Max generator loss : 1494.003173828125\n",
      "Min generator loss : 1.0104471445083618\n",
      "Generator loss decrease : -7.311101913452148(2.2523114681243896)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 8:48:43.653733\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:24 ETA:   0:00:00{\n",
      "2021-05-23 18:47:14,192 - train - INFO - \n",
      "Epoch : 23/200 (30)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 7.293735504150391\n",
      "Max generator loss : 256.8729553222656\n",
      "Min generator loss : 0.7311204075813293\n",
      "Generator loss decrease : -1.4556493759155273(1.2493367195129395)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 9:10:09.013830\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:20 ETA:   0:00:00{\n",
      "2021-05-23 19:08:35,978 - train - INFO - \n",
      "Epoch : 24/200 (32)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.45258617401123\n",
      "Max generator loss : 918.3782348632812\n",
      "Min generator loss : 0.7463728785514832\n",
      "Generator loss decrease : -4.614500045776367(1.7904131412506104)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 9:31:30.799362\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:19 ETA:   0:00:00{\n",
      "2021-05-23 19:29:57,336 - train - INFO - \n",
      "Epoch : 25/200 (34)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 5.206923007965088\n",
      "Max generator loss : 31.23622703552246\n",
      "Min generator loss : 0.7321934103965759\n",
      "Generator loss decrease : 0.6311631202697754(0.8918886780738831)\n",
      "Current lowest generator loss : 5.838086128234863\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 9:52:52.157334\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss : 0.02019948698580265\n",
      "generator_discriminator_loss : 0.020084761083126068\n",
      "train_f1_loss : 0.04425465688109398\n",
      "valid_f1_loss : 0.7843296527862549\n",
      "current/min total_f1_loss = 0.8285843133926392 / 0.2531275749206543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 4841) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:20 ETA:   0:00:00{\n",
      "2021-05-23 19:51:33,732 - train - INFO - \n",
      "Epoch : 26/200 (35)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 6.341499328613281\n",
      "Max generator loss : 605.4688720703125\n",
      "Min generator loss : 0.7450565695762634\n",
      "Generator loss decrease : -1.1345763206481934(1.217897653579712)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 10:14:28.553732\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:24 ETA:   0:00:00{\n",
      "2021-05-23 20:12:59,639 - train - INFO - \n",
      "Epoch : 27/200 (37)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.908289909362793\n",
      "Max generator loss : 257.17279052734375\n",
      "Min generator loss : 0.7158710956573486\n",
      "Generator loss decrease : -5.701366901397705(2.094959020614624)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 10:35:54.460938\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:23 ETA:   0:00:00{\n",
      "2021-05-23 20:34:24,596 - train - INFO - \n",
      "Epoch : 28/200 (39)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.875265121459961\n",
      "Max generator loss : 1790.885009765625\n",
      "Min generator loss : 0.7015907764434814\n",
      "Generator loss decrease : -5.668342113494873(2.088616371154785)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.004\n",
      "Elapsed_time : 10:57:19.417438\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:06 ETA:   0:00:00{\n",
      "2021-05-23 20:55:31,995 - train - INFO - \n",
      "Epoch : 29/200 (41)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 11.307196617126465\n",
      "Max generator loss : 570.3700561523438\n",
      "Min generator loss : 0.7084463238716125\n",
      "Generator loss decrease : -6.100273609161377(2.17156982421875)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 11:18:26.816968\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:03 ETA:   0:00:00{\n",
      "2021-05-23 21:16:36,918 - train - INFO - \n",
      "Epoch : 30/200 (43)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 6.2381086349487305\n",
      "Max generator loss : 180.81625366210938\n",
      "Min generator loss : 0.6725919246673584\n",
      "Generator loss decrease : -1.0311856269836426(1.198041319847107)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 11:39:31.739566\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:17 ETA:   0:00:00{\n",
      "2021-05-23 21:37:55,332 - train - INFO - \n",
      "Epoch : 31/200 (45)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 15.681488990783691\n",
      "Max generator loss : 2524.70458984375\n",
      "Min generator loss : 0.7368347644805908\n",
      "Generator loss decrease : -10.474565505981445(3.0116612911224365)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 12:00:50.153593\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:27 ETA:   0:00:00{\n",
      "2021-05-23 21:59:24,208 - train - INFO - \n",
      "Epoch : 32/200 (47)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 10.5906982421875\n",
      "Max generator loss : 573.6749877929688\n",
      "Min generator loss : 0.8025757074356079\n",
      "Generator loss decrease : -5.383775234222412(2.0339648723602295)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 12:22:19.029656\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:19 ETA:   0:00:00{\n",
      "2021-05-23 22:20:45,313 - train - INFO - \n",
      "Epoch : 33/200 (49)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 6.003851413726807\n",
      "Max generator loss : 236.2362518310547\n",
      "Min generator loss : 0.6557297706604004\n",
      "Generator loss decrease : -0.7969284057617188(1.153051733970642)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 12:43:40.135082\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:17 ETA:   0:00:00{\n",
      "2021-05-23 22:42:05,006 - train - INFO - \n",
      "Epoch : 34/200 (51)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 8.573445320129395\n",
      "Max generator loss : 355.2560119628906\n",
      "Min generator loss : 0.704757809638977\n",
      "Generator loss decrease : -3.3665223121643066(1.6465473175048828)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 13:04:59.827699\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:17 ETA:   0:00:00{\n",
      "2021-05-23 23:03:23,967 - train - INFO - \n",
      "Epoch : 35/200 (53)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 19.916030883789062\n",
      "Max generator loss : 2405.467041015625\n",
      "Min generator loss : 0.7227320075035095\n",
      "Generator loss decrease : -14.709108352661133(3.824913740158081)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 13:26:18.788692\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:17 ETA:   0:00:00{\n",
      "2021-05-23 23:24:42,558 - train - INFO - \n",
      "Epoch : 36/200 (55)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 12.919230461120605\n",
      "Max generator loss : 547.5027465820312\n",
      "Min generator loss : 0.6298404932022095\n",
      "Generator loss decrease : -7.712307453155518(2.4811642169952393)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 13:47:37.379884\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:18 ETA:   0:00:00{\n",
      "2021-05-23 23:46:02,733 - train - INFO - \n",
      "Epoch : 37/200 (57)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 14.537243843078613\n",
      "Max generator loss : 653.4037475585938\n",
      "Min generator loss : 0.7214819192886353\n",
      "Generator loss decrease : -9.330320358276367(2.7919068336486816)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 14:08:57.554569\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:17 ETA:   0:00:00{\n",
      "2021-05-24 00:07:21,280 - train - INFO - \n",
      "Epoch : 38/200 (59)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 8.974567413330078\n",
      "Max generator loss : 875.7852783203125\n",
      "Min generator loss : 0.6585650444030762\n",
      "Generator loss decrease : -3.7676444053649902(1.723583698272705)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 0.0004\n",
      "Elapsed_time : 14:30:16.101660\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:04 ETA:   0:00:00{\n",
      "2021-05-24 00:28:26,889 - train - INFO - \n",
      "Epoch : 39/200 (61)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 14.476217269897461\n",
      "Max generator loss : 595.5435180664062\n",
      "Min generator loss : 0.707314133644104\n",
      "Generator loss decrease : -9.269294738769531(2.780186653137207)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 4.000000000000001e-05\n",
      "Elapsed_time : 14:51:21.710306\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:04 ETA:   0:00:00{\n",
      "2021-05-24 00:49:32,284 - train - INFO - \n",
      "Epoch : 40/200 (63)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 12.526894569396973\n",
      "Max generator loss : 612.84765625\n",
      "Min generator loss : 0.7450987696647644\n",
      "Generator loss decrease : -7.319971561431885(2.4058151245117188)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 4.000000000000001e-05\n",
      "Elapsed_time : 15:12:27.105597\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:13 ETA:   0:00:00{\n",
      "2021-05-24 01:10:46,586 - train - INFO - \n",
      "Epoch : 41/200 (65)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 8.851781845092773\n",
      "Max generator loss : 212.49334716796875\n",
      "Min generator loss : 0.7492866516113281\n",
      "Generator loss decrease : -3.6448588371276855(1.7000024318695068)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 4.000000000000001e-05\n",
      "Elapsed_time : 15:33:41.406640\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:07 ETA:   0:00:00{\n",
      "2021-05-24 01:31:55,444 - train - INFO - \n",
      "Epoch : 42/200 (67)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 5.884045600891113\n",
      "Max generator loss : 182.96917724609375\n",
      "Min generator loss : 0.7031634449958801\n",
      "Generator loss decrease : -0.6771225929260254(1.1300427913665771)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 4.000000000000001e-05\n",
      "Elapsed_time : 15:54:50.266023\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (4840 of 4841) |################### | Elapsed Time: 0:21:05 ETA:   0:00:00{\n",
      "2021-05-24 01:53:02,445 - train - INFO - \n",
      "Epoch : 43/200 (69)\n",
      "Discriminator_acces : 0.0\n",
      "Mean generator loss : 9.74993896484375\n",
      "Max generator loss : 1500.2706298828125\n",
      "Min generator loss : 0.7423070073127747\n",
      "Generator loss decrease : -4.543015956878662(1.8724952936172485)\n",
      "Current lowest generator loss : 5.206923007965088\n",
      "Current Learning_rate : 4.000000000000001e-05\n",
      "Elapsed_time : 16:15:57.266310\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss decrease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59% (2864 of 4841) |###########         | Elapsed Time: 0:12:31 ETA:   0:08:54ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-3ceeaa4d0883>\", line 3, in <module>\n",
      "    gan.train(epochs=200, batch_size=batch_size, epoch_shuffle_term=50)\n",
      "  File \"<ipython-input-1-399c1c085fa3>\", line 317, in train\n",
      "    generator_loss = self.combined.train_on_batch(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1698, in train_on_batch\n",
      "    self.reset_metrics()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1637, in reset_metrics\n",
      "    m.reset_states()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 247, in reset_states\n",
      "    K.batch_set_value([(v, 0) for v in self.variables])\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3576, in batch_set_value\n",
      "    x.assign(np.asarray(value, dtype=dtype(x)))\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 859, in assign\n",
      "    assign_op = gen_resource_variable_ops.assign_variable_op(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 142, in assign_variable_op\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-3ceeaa4d0883>\", line 3, in <module>\n",
      "    gan.train(epochs=200, batch_size=batch_size, epoch_shuffle_term=50)\n",
      "  File \"<ipython-input-1-399c1c085fa3>\", line 317, in train\n",
      "    generator_loss = self.combined.train_on_batch(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1698, in train_on_batch\n",
      "    self.reset_metrics()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1637, in reset_metrics\n",
      "    m.reset_states()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 247, in reset_states\n",
      "    K.batch_set_value([(v, 0) for v in self.variables])\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3576, in batch_set_value\n",
      "    x.assign(np.asarray(value, dtype=dtype(x)))\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 859, in assign\n",
      "    assign_op = gen_resource_variable_ops.assign_variable_op(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 142, in assign_variable_op\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3435, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-3ceeaa4d0883>\", line 3, in <module>\n",
      "    gan.train(epochs=200, batch_size=batch_size, epoch_shuffle_term=50)\n",
      "  File \"<ipython-input-1-399c1c085fa3>\", line 317, in train\n",
      "    generator_loss = self.combined.train_on_batch(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1698, in train_on_batch\n",
      "    self.reset_metrics()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1637, in reset_metrics\n",
      "    m.reset_states()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 247, in reset_states\n",
      "    K.batch_set_value([(v, 0) for v in self.variables])\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3576, in batch_set_value\n",
      "    x.assign(np.asarray(value, dtype=dtype(x)))\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 859, in assign\n",
      "    assign_op = gen_resource_variable_ops.assign_variable_op(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 142, in assign_variable_op\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3435, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\gr300\\anaconda3\\lib\\ntpath.py\", line 647, in realpath\n",
      "    path = _getfinalpathname(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# gan.load_study_info()\n",
    "# gan.start_epoch = 8\n",
    "gan.train(epochs=200, batch_size=batch_size, epoch_shuffle_term=50)\n",
    "# gan.train(epochs=20, batch_size=batch_size, epoch_shuffle_term=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 5\n",
    "\n",
    "gan.loaded_data_index[\"train\"][4840:4845]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gan.load_study_info()\n",
    "# gan.start_epoch = 8\n",
    "gan.train(epochs=200, batch_size=batch_size, epoch_shuffle_term=50)\n",
    "# gan.train(epochs=20, batch_size=batch_size, epoch_shuffle_term=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator_losses[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_patch = np.ones(\n",
    "    (gan.batch_size, *gan.disc_patch), dtype=np.float32)\n",
    "\n",
    "generator_loss = gan.combined.train_on_batch(\n",
    "    gan.original_img,\n",
    "    [valid_patch, gan.masked_img],\n",
    "    class_weight={0: 0.1, 1: 0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i=0\n",
    "batch_index = gan.loaded_data_index[\"train\"][batch_i: batch_i +\n",
    "                                              gan.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(gan.generator_losses[batch_index]) > np.quantile(gan.generator_losses, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan.load_study_info()\n",
    "#gan.start_epoch = 30\n",
    "# gan.start_epoch = 2 \n",
    "gan.train(epochs=325, batch_size=batch_size, epoch_shuffle_term=100)\n",
    "# gan.train(epochs=20, batch_size=batch_size, epoch_shuffle_term=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight = gan.generator.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in temp_weight:\n",
    "    if \"dense\" in weight.name:\n",
    "        if \"bias\" in weight.name:\n",
    "            print(weight.name)\n",
    "            print(weight.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight = gan.discriminator.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in temp_weight:\n",
    "    if \"dense\" in weight.name:\n",
    "        if \"bias\" in weight.name:\n",
    "            print(weight.name)\n",
    "            print(weight.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.original_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(gan.original_img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(gan.original_img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gan.original_img.numpy() + 1) * 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = gan.original_img.numpy()\n",
    "predicted = gan.generator.predict_on_batch(image)\n",
    "mask = gan.masked_img.numpy()\n",
    "\n",
    "image = ((image + 1) * 127.5).astype('uint8')\n",
    "predicted = ((predicted + 1) * 127.5).astype('uint8')\n",
    "mask = ((mask + 1) * 127.5).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(image[index])\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(predicted[index])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(mask[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(gan.original_img))\n",
    "print(np.min(gan.original_img))\n",
    "print(np.max(temp))\n",
    "print(np.min(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gan.data_loader.loaded_data_object[\"train\"].values()\n",
    "\n",
    "for index, (input_img, output_img) in enumerate(zip(*temp)):\n",
    "    print(index)\n",
    "    if index > 40:\n",
    "        break\n",
    "    print(index)\n",
    "    print(input_img.shape)\n",
    "    print(output_img.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "temp_source = gan.original_img\n",
    "temp_mask = gan.masked_img\n",
    "\n",
    "start_time = time.time()\n",
    "gan.generator.train_on_batch(temp_source, temp_mask)\n",
    "print(f\"elapsed time : {time.time() - start_time}\")\n",
    "\n",
    "temp_source = tf.convert_to_tensor(temp_source)\n",
    "temp_mask = tf.convert_to_tensor(temp_mask)\n",
    "\n",
    "start_time = time.time()\n",
    "gan.generator.train_on_batch(temp_source, temp_mask)\n",
    "print(f\"elapsed time : {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator : 260초\n",
    "# Queue Iterator : 200초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "ITER_NUM = 620\n",
    "batch_size = 10\n",
    "\n",
    "gan.generator.compile(\n",
    "    loss=sm.losses.BinaryFocalLoss(),\n",
    "    optimizer=Nadam(gan.generator_learning_rate),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "def batch_setter(queue):\n",
    "    batch_i = 0\n",
    "    count = 0\n",
    "    while batch_i + gan.batch_size <= gan.data_loader.train_data_length and count < ITER_NUM:\n",
    "        \n",
    "        batch_index = gan.train_loaded_data_index[batch_i: batch_i +\n",
    "                                                   gan.batch_size]        \n",
    "        \n",
    "        batch_tuple = gan.data_loader.get_data(\n",
    "        data_mode=\"train\", index=batch_index)\n",
    "\n",
    "        queue.put(batch_tuple)\n",
    "        queue.join()\n",
    "        count += 1\n",
    "    \n",
    "def batch_getter(queue):\n",
    "    \n",
    "    original_img, masked_img = queue.get()\n",
    "    tensor_original_img = tf.convert_to_tensor(original_img)\n",
    "    tensor_masked_img = tf.convert_to_tensor(masked_img)\n",
    "    queue.task_done()\n",
    "    \n",
    "    return tensor_original_img, tensor_masked_img\n",
    "    \n",
    "def batch_trainer(original_img, masked_img):\n",
    "    \n",
    "    gan.generator.train_on_batch(temp_source, temp_mask)\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "setter = threading.Thread(target=batch_setter, args=(q,),daemon=True)\n",
    "setter.start()\n",
    "start_time = time.time()\n",
    "for i in range(ITER_NUM):\n",
    "    tensor_original_img, tensor_masked_img = batch_getter(q)\n",
    "    \n",
    "    gan.generator.train_on_batch(tensor_original_img, tensor_masked_img)\n",
    "print(f\"elapsed time : {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "batch_i = 0\n",
    "count = 0\n",
    "while batch_i + gan.batch_size <= gan.data_loader.train_data_length and count < ITER_NUM:\n",
    "\n",
    "    batch_index = gan.train_loaded_data_index[batch_i: batch_i +\n",
    "                                               gan.batch_size]        \n",
    "    batch_tuple = gan.data_loader.get_data(\n",
    "    data_mode=\"train\", index=batch_index)\n",
    "    \n",
    "    gan.generator.train_on_batch(*batch_tuple)\n",
    "    \n",
    "    count += 1\n",
    "print(f\"elapsed time : {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "temp = tensor_masked_img\n",
    "print(type(temp))\n",
    "print(type(temp.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(temp.numpy(), tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_module.model.build_model import build_dual_discriminator\n",
    "\n",
    "temp = build_dual_discriminator(\n",
    "            input_img_shape=(512,512,3),\n",
    "            output_img_shape=(512,512,1),\n",
    "            discriminator_power=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "temp.compile(\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n",
    "        tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)\n",
    "    ],\n",
    "    optimizer=Nadam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_mockup = np.ones((1,512,512,3))\n",
    "mask_mockup = np.ones((1,512,512,1))\n",
    "patch_mockup = np.ones((1,8,8,1))\n",
    "\n",
    "temp.test_on_batch([image_mockup, mask_mockup], [patch_mockup, patch_mockup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
