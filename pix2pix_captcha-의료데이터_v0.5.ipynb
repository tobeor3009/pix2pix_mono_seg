{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 75%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "# TBD 1 : logger 추가\n",
    "# TBD 2: flask github 참고, method, class, 파일의 맨 윗단 마다 pydoc 형식으로 달기\n",
    "# TBD 3: 축약어를 자제할것 (특히 변수)\n",
    "\n",
    "# tensorflow Module\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as keras_backend\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "# python basic Module\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "from datetime import datetime\n",
    "from shutil import copy\n",
    "from pickle import dump, load\n",
    "\n",
    "# math, image, plot Module\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt  # TBD\n",
    "\n",
    "# email Module\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.header import Header\n",
    "\n",
    "from data_loader.medical_segmentation_data_loader_v1 import DataLoader\n",
    "\n",
    "from gan_module.model import build_generator, build_discriminator\n",
    "from gan_module.draw_images import ImageDrawer\n",
    "from gan_module.custom_loss import f1_loss_for_training, f1_score, dice_loss_for_training\n",
    "\n",
    "# set GPU memory growth allocation\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\"\"\"\n",
    "if you don't have nvidia-gpu, try plaidml! but it will works tensorflow 1.x.x\n",
    "# pip install -U plaidml-keras\n",
    "# plaidml-setup\n",
    "\"\"\"\n",
    "# use_plaidml = False\n",
    "# if use_plaidml :\n",
    "#     import plaidml.keras\n",
    "#     plaidml.keras.install_backend()\n",
    "#     os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "class Pix2PixSegmentation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_power=32,\n",
    "        discriminator_power=32,\n",
    "        generator_learning_rate=1e-4,\n",
    "        discriminator_learning_rate=1e-4,\n",
    "        find_error=False,\n",
    "        temp_weights_path=\".\",\n",
    "        draw_images=True,\n",
    "        on_memory=True,\n",
    "        test=False\n",
    "    ):\n",
    "        # smtp info\n",
    "        self.smtp_host = \"smtp.gmail.com\"\n",
    "        self.smtp_port = 465\n",
    "        self.smtp_id = \"rpa.manager0001@gmail.com\"\n",
    "        self.smtp_password = \"!rpa.admin!23\"\n",
    "        self.smtp_to_addr = \"tobeor3009@gmail.com\"\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 512\n",
    "        self.img_cols = 512\n",
    "        self.input_channels = 3\n",
    "        self.output_channels = 1\n",
    "        self.input_img_shape = (\n",
    "            self.img_rows, self.img_cols, self.input_channels)\n",
    "        self.output_img_shape = (\n",
    "            self.img_rows, self.img_cols, self.output_channels)\n",
    "        # set parameter\n",
    "        self.start_epoch = None\n",
    "        self.history = [[], [], []]\n",
    "        self.f1_loss_ratio = 16\n",
    "        self.huber_loss_ratio = 64\n",
    "        self.find_error = find_error\n",
    "        self.find_error_epoch = 30\n",
    "        self.error_list = []\n",
    "        self.temp_weights_path = temp_weights_path\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = \"tumor\"\n",
    "        self.data_loader = DataLoader(\n",
    "            dataset_name=self.dataset_name,\n",
    "            img_res=(self.img_rows, self.img_cols),\n",
    "            on_memory=on_memory, test=test\n",
    "        )\n",
    "        self.train_loaded_data, self.valid_loaded_data = self.data_loader.load_all()\n",
    "        if test:\n",
    "            self.train_loaded_data_len = 20\n",
    "            self.valid_loaded_data_len = 20\n",
    "        else:\n",
    "            self.train_loaded_data_len = self.data_loader.train_data_length\n",
    "            self.valid_loaded_data_len = self.data_loader.valid_data_length\n",
    "\n",
    "        self.train_loaded_data_index = np.arange(self.train_loaded_data_len)\n",
    "        self.valid_loaded_data_index = np.arange(self.valid_loaded_data_len)\n",
    "\n",
    "        # Configure Image Drawer\n",
    "        self.draw_images = draw_images\n",
    "        self.image_drawer = ImageDrawer(\n",
    "            dataset_name=self.dataset_name, data_loader=self.data_loader\n",
    "        )\n",
    "        # training parameters\n",
    "        self.learning_schedule = [\n",
    "            50,\n",
    "            100,\n",
    "            150,\n",
    "            200,\n",
    "            250,\n",
    "            300,\n",
    "            350,\n",
    "            400,\n",
    "            450,\n",
    "            500,\n",
    "            550\n",
    "        ]\n",
    "        self.discriminator_acc_previous = 0.5\n",
    "        self.discriminator_acc_high_indexes = np.array(\n",
    "            [0.5 for _ in range(self.train_loaded_data_len)])\n",
    "        self.discriminator_acc_high_indexes_previous = self.discriminator_acc_high_indexes.copy()\n",
    "        self.generator_loss_min = 100\n",
    "        self.generator_loss_previous = 100\n",
    "        self.generator_loss_max_previous = 1000\n",
    "        self.generator_loss_max_min = 1000\n",
    "        self.generator_loss_min_min = 1000\n",
    "        self.weight_save_stack = False\n",
    "        self.training_end_stack = 0\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2 ** 2)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.generator_power = generator_power\n",
    "        self.discriminator_power = discriminator_power\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "        generator_optimizer = Nadam(self.generator_learning_rate)\n",
    "        discriminator_optimizer = Nadam(self.discriminator_learning_rate)\n",
    "\n",
    "        # layer Component\n",
    "        self.kernel_initializer = RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = build_generator(\n",
    "            input_img_shape=self.input_img_shape,\n",
    "            output_channels=self.output_channels,\n",
    "            generator_power=self.generator_power,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "        )        \n",
    "        self.generator.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=generator_optimizer,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator(\n",
    "            input_img_shape=self.input_img_shape,\n",
    "            output_img_shape=self.output_img_shape,\n",
    "            discriminator_power=self.discriminator_power,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "        )\n",
    "        # self.discriminator = self.build_discriminator()\n",
    "        # 'mse' or tf.keras.losses.Huber() tf.keras.losses.LogCosh()\n",
    "        self.discriminator.compile(\n",
    "            loss=tf.keras.losses.LogCosh(),\n",
    "            optimizer=discriminator_optimizer,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        # -------------------------\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        original_img = Input(shape=self.input_img_shape)\n",
    "        masked_img = Input(shape=self.output_img_shape)\n",
    "        # generate image from original_img for target masked_img\n",
    "        model_masked_img = self.generator(original_img)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        model_validity = self.discriminator([original_img, model_masked_img])\n",
    "        # give score by\n",
    "        # 1. how generator trick discriminator\n",
    "        # 2. how generator's image same as real photo in pixel\n",
    "        # 3. if you want change loss, see doc https://keras.io/api/losses/\n",
    "        # 4. 'mse', 'mae', tf.keras.losses.LogCosh(),  tf.keras.losses.Huber()\n",
    "        self.combined = Model(\n",
    "            inputs=[original_img, masked_img],\n",
    "            outputs=[model_validity, model_masked_img, model_masked_img],\n",
    "        )\n",
    "        self.combined.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.LogCosh(),\n",
    "                sm.losses.BinaryFocalLoss(),\n",
    "                dice_loss_for_training\n",
    "            ],\n",
    "            loss_weights=[2, self.huber_loss_ratio, self.f1_loss_ratio],\n",
    "            optimizer=generator_optimizer\n",
    "        )\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        self.training_end_stack = 0\n",
    "        self.batch_size = batch_size\n",
    "        valid_patch = np.ones((self.batch_size,) +\n",
    "                              self.disc_patch, dtype=np.float32)\n",
    "        fake_patch = np.zeros((self.batch_size,) +\n",
    "                              self.disc_patch, dtype=np.float32)\n",
    "        if self.start_epoch is None:\n",
    "            self.start_epoch = 0\n",
    "        for epoch in range(self.start_epoch, epochs):\n",
    "            batch_i = 0\n",
    "            discriminator_losses = []\n",
    "            discriminator_acces = []\n",
    "            train_generator_loss = []\n",
    "            generator_loss_max_in_epoch = 0\n",
    "            generator_loss_min_in_epoch = 1000\n",
    "            generator_current_learning_rate = self.learning_rate_scheduler(\n",
    "                self.generator_learning_rate,\n",
    "                epoch,\n",
    "            )\n",
    "            discriminator_current_learning_rate = self.learning_rate_scheduler(\n",
    "                self.discriminator_learning_rate,\n",
    "                epoch,\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.combined.optimizer.learning_rate, generator_current_learning_rate\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.discriminator.optimizer.learning_rate,\n",
    "                discriminator_current_learning_rate,\n",
    "            )\n",
    "            # shffle data 5 epoch term\n",
    "            if epoch % 5 == 0 and not isinstance(self.train_loaded_data, types.GeneratorType):\n",
    "                np.random.shuffle(self.train_loaded_data_index)\n",
    "                self.train_loaded_data = [\n",
    "                    self.train_loaded_data[i][self.train_loaded_data_index] for i in range(2)\n",
    "                ]\n",
    "                self.discriminator_acc_high_indexes_previous = self.discriminator_acc_high_indexes_previous[\n",
    "                    self.train_loaded_data_index]\n",
    "            if self.discriminator_acc_previous < 0.8:\n",
    "                discriminator_learning = True\n",
    "                print(\"discriminator_learning is True\")\n",
    "            else:\n",
    "                discriminator_learning = False\n",
    "                print(\"discriminator_learning is False\")\n",
    "\n",
    "            while batch_i + self.batch_size <= self.train_loaded_data_len:\n",
    "                original_img = self.train_loaded_data[0][batch_i: batch_i +\n",
    "                                                         self.batch_size]\n",
    "                masked_img = self.train_loaded_data[1][batch_i: batch_i +\n",
    "                                                       self.batch_size]\n",
    "                model_masked_img = self.generator.predict_on_batch(\n",
    "                    original_img)\n",
    "\n",
    "                # forTest\n",
    "                self.masked_img = masked_img\n",
    "                self.original_img = original_img\n",
    "                self.model_masked_img = model_masked_img\n",
    "                self.valid_path = valid_patch\n",
    "                self.fake_patch = fake_patch\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train Discriminator for valid image if it failed to detect fake image\n",
    "                if discriminator_learning:\n",
    "                    self.discriminator.train_on_batch(\n",
    "                        [original_img, masked_img], valid_patch)\n",
    "                discriminator_acc_previos = np.mean(self.discriminator_acc_high_indexes_previous[batch_i: batch_i + self.batch_size])\n",
    "                \n",
    "                # Train generator for image detected by discriminator\n",
    "                if discriminator_acc_previos > 0.5 and epoch > 1:\n",
    "                    self.generator.train_on_batch(\n",
    "                        original_img, masked_img)\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                generator_loss = self.combined.train_on_batch(\n",
    "                    [original_img, masked_img],\n",
    "                    [valid_patch, masked_img, masked_img],\n",
    "                )\n",
    "                # train discriminator for fake image if it failed to detect fake image\n",
    "                if discriminator_acc_previos <= 0.5:\n",
    "                    discriminator_loss = self.discriminator.train_on_batch(\n",
    "                        [original_img, model_masked_img], fake_patch)\n",
    "                else:\n",
    "                    discriminator_loss = self.discriminator.test_on_batch(\n",
    "                        [original_img, model_masked_img], fake_patch)\n",
    "                \n",
    "                self.discriminator_acc_high_indexes[batch_i: batch_i +\n",
    "                                                    self.batch_size] = discriminator_loss[1]\n",
    "                elapsed_time = datetime.now() - start_time\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    # Plot the progress\n",
    "                    print(\n",
    "                        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\"\n",
    "                        % (\n",
    "                            epoch,\n",
    "                            epochs,\n",
    "                            batch_i,\n",
    "                            self.train_loaded_data_len,\n",
    "                            discriminator_loss[0],\n",
    "                            100 * discriminator_loss[1],\n",
    "                            generator_loss[0],\n",
    "                            elapsed_time,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0 and self.draw_images:\n",
    "                    self.image_drawer.sample_images(\n",
    "                        self.generator, epoch, batch_i)\n",
    "\n",
    "                discriminator_losses.append(discriminator_loss[0])\n",
    "                discriminator_acces.append(discriminator_loss[1])\n",
    "                train_generator_loss.append(generator_loss[0])\n",
    "                # loss 가 가장 높은 이미지를 저장 및 max_in_epoch 갱신\n",
    "                if generator_loss[0] > generator_loss_max_in_epoch:\n",
    "                    model_masked_img = self.generator.predict_on_batch(\n",
    "                        original_img)\n",
    "                    if self.draw_images:\n",
    "                        self.image_drawer.draw_worst_and_best(\n",
    "                            original_img,\n",
    "                            model_masked_img,\n",
    "                            masked_img,\n",
    "                            epoch,\n",
    "                            worst=True,\n",
    "                        )\n",
    "                    generator_loss_max_in_epoch = generator_loss[0]\n",
    "                # loss 가 가장 낮은 이미지를 저장 및 max_in_epoch 갱신\n",
    "                if generator_loss_min_in_epoch > generator_loss[0]:\n",
    "                    model_masked_img = self.generator.predict_on_batch(\n",
    "                        original_img)\n",
    "                    if self.draw_images:\n",
    "                        self.image_drawer.draw_worst_and_best(\n",
    "                            original_img,\n",
    "                            model_masked_img,\n",
    "                            masked_img,\n",
    "                            epoch,\n",
    "                            worst=False,\n",
    "                        )\n",
    "                    generator_loss_min_in_epoch = generator_loss[0]\n",
    "\n",
    "                # 한 배치 끝\n",
    "                batch_i += self.batch_size\n",
    "            # training batch 사이클 끝\n",
    "            self.history[0].append(discriminator_loss[0])\n",
    "            self.history[1].append(generator_loss[0])\n",
    "            self.history[2].append(100 * discriminator_loss[1])\n",
    "            print(f\"discriminator_acces : {np.mean(discriminator_acces)}\")\n",
    "            print(\n",
    "                f\"Mean generator_loss : {np.mean(train_generator_loss)}\")\n",
    "            print(f\"Max generator_loss : {np.max(train_generator_loss)}\")\n",
    "            print(f\"Min generator_loss : {np.min(train_generator_loss)}\")\n",
    "            print(\n",
    "                f\"generator loss decrease : {self.generator_loss_previous - np.mean(train_generator_loss)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"generator loss decrease ratio : ({np.mean(train_generator_loss) / self.generator_loss_previous})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Max generator loss decrease : {self.generator_loss_max_previous - np.max(train_generator_loss)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"current lowest generator loss : {self.generator_loss_min}\")\n",
    "            print(\n",
    "                f\"current Learning_rate = {generator_current_learning_rate}\")\n",
    "            # rollback if loss not converge\n",
    "            if np.mean(train_generator_loss) / self.generator_loss_min < 1.02:\n",
    "                if self.generator_loss_min > np.mean(train_generator_loss):\n",
    "                    self.generator_loss_min = np.mean(train_generator_loss)\n",
    "                    self.generator_loss_max_min = generator_loss_max_in_epoch\n",
    "                    self.generator_loss_min_min = generator_loss_min_in_epoch\n",
    "                    self.weight_save_stack = True\n",
    "                    self.save_study_info()\n",
    "                    print(\"save weights\")\n",
    "            else:\n",
    "                self.load_best_weights()\n",
    "\n",
    "            # previous generator_loss 갱신\n",
    "            self.generator_loss_previous = np.mean(train_generator_loss)\n",
    "            self.generator_loss_max_previous = generator_loss_max_in_epoch\n",
    "\n",
    "            if epoch >= 10 and self.weight_save_stack:\n",
    "                copy(\n",
    "                    \"generator.h5\",\n",
    "                    \"./generator_weights/generator_\"\n",
    "                    + str(round(self.generator_loss_min, 5))\n",
    "                    + \"_\"\n",
    "                    + str(round(self.generator_loss_max_min, 5))\n",
    "                    + \".h5\",\n",
    "                )\n",
    "                self.weight_save_stack = False\n",
    "\n",
    "            self.discriminator_acc_previous = np.mean(discriminator_acces)\n",
    "            self.discriminator_acc_high_indexes_previous = self.discriminator_acc_high_indexes.copy()\n",
    "            train_f1_loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            train_predict_mini_batch_size = 1\n",
    "            for index in range(0, self.train_loaded_data_len, train_predict_mini_batch_size):\n",
    "\n",
    "                train_original_img = self.train_loaded_data[0][index:index +\n",
    "                                                               train_predict_mini_batch_size]\n",
    "                train_masked_img = self.train_loaded_data[1][index:index +\n",
    "                                                             train_predict_mini_batch_size]\n",
    "                train_model_masked_img = self.generator.predict_on_batch(\n",
    "                    self.train_loaded_data[0][index:index + train_predict_mini_batch_size])\n",
    "\n",
    "                train_f1_loss = f1_loss_for_training(\n",
    "                    train_masked_img, np.squeeze(train_model_masked_img))\n",
    "                train_f1_score = f1_score(\n",
    "                    train_masked_img, np.squeeze(train_model_masked_img))\n",
    "                train_f1_loss_list.append(train_f1_loss)\n",
    "                train_f1_score_list.append(train_f1_score)\n",
    "            print(\n",
    "                f\"train_f1_loss : {np.mean(train_f1_loss_list) * self.f1_loss_ratio}\")\n",
    "            print(f\"train_f1_score : {1 - np.mean(train_f1_loss_list)}\")\n",
    "            print(f\"train_f1_rounded_score : {np.mean(train_f1_score_list)}\")\n",
    "\n",
    "            valid_f1_loss_list = []\n",
    "            valid_f1_score_list = []\n",
    "            valid_predict_mini_batch_size = 1\n",
    "            for index in range(0, self.valid_loaded_data_len, valid_predict_mini_batch_size):\n",
    "\n",
    "                valid_original_img = self.valid_loaded_data[0][index:index +\n",
    "                                                               valid_predict_mini_batch_size]\n",
    "                valid_masked_img = self.valid_loaded_data[1][index:index +\n",
    "                                                             valid_predict_mini_batch_size]\n",
    "                valid_model_masked_img = self.generator.predict_on_batch(\n",
    "                    self.valid_loaded_data[0][index:index + valid_predict_mini_batch_size])\n",
    "\n",
    "                valid_f1_loss = f1_loss_for_training(\n",
    "                    valid_masked_img, np.squeeze(valid_model_masked_img))\n",
    "                valid_f1_score = f1_score(\n",
    "                    valid_masked_img, np.squeeze(valid_model_masked_img))\n",
    "                valid_f1_loss_list.append(valid_f1_loss)\n",
    "                valid_f1_score_list.append(valid_f1_score)\n",
    "            print(\n",
    "                f\"valid_f1_loss : {np.mean(valid_f1_loss_list) * self.f1_loss_ratio}\")\n",
    "            print(f\"valid_f1_score : {1 - np.mean(valid_f1_loss_list)}\")\n",
    "            print(f\"valid_f1_rounded_score : {np.mean(valid_f1_score_list)}\")\n",
    "\n",
    "    def learning_rate_scheduler(self, learning_rate, epoch):\n",
    "\n",
    "        exponent = 0.9\n",
    "        max_epoch = 575\n",
    "        new_learning_rate = learning_rate * (1 - epoch / max_epoch)**exponent\n",
    "\n",
    "        return new_learning_rate\n",
    "\n",
    "    def get_info_folderPath(self):\n",
    "        return (\n",
    "            str(round(self.generator_loss_min, 5))\n",
    "            + \"_\"\n",
    "            + str(round(self.generator_loss_max_min, 5))\n",
    "        )\n",
    "\n",
    "    def save_study_info(self, path=None):\n",
    "\n",
    "        if path == None:\n",
    "            path = self.temp_weights_path\n",
    "\n",
    "        generator_weigth_path = os.path.join(path, \"generator.h5\")\n",
    "        discriminator_weigth_path = os.path.join(path, \"discriminator.h5\")\n",
    "        combined_weigth_path = os.path.join(path, \"combined.h5\")\n",
    "\n",
    "        self.generator.save_weights(generator_weigth_path)\n",
    "        self.discriminator.save_weights(discriminator_weigth_path)\n",
    "        self.combined.save_weights(combined_weigth_path)\n",
    "\n",
    "        study_info = {}\n",
    "        study_info[\"start_epoch\"] = self.start_epoch\n",
    "        study_info[\"generator_loss_min\"] = self.generator_loss_min\n",
    "        study_info[\"generator_loss_max_min\"] = self.generator_loss_max_min\n",
    "        study_info[\"generator_loss_min_min\"] = self.generator_loss_min_min\n",
    "\n",
    "        file = open(path + \"/study_info.pkl\", \"wb\")\n",
    "        dump(study_info, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_best_weights(self):\n",
    "        self.generator.load_weights(self.temp_weights_path + \"/generator.h5\")\n",
    "        self.discriminator.load_weights(\n",
    "            self.temp_weights_path + \"/discriminator.h5\")\n",
    "        self.combined.load_weights(self.temp_weights_path + \"/combined.h5\")\n",
    "\n",
    "    def load_study_info(self):\n",
    "\n",
    "        self.generator.load_weights(\"generator.h5\")\n",
    "        self.discriminator.load_weights(\"discriminator.h5\")\n",
    "        self.combined.load_weights(\"combined.h5\")\n",
    "\n",
    "        if os.path.isfile(\"study_info.pkl\"):\n",
    "            file = open(\"study_info.pkl\", \"rb\")\n",
    "            study_info = load(file)\n",
    "            file.close()\n",
    "            self.start_epoch = study_info[\"start_epoch\"]\n",
    "            self.generator_loss_min = study_info[\"generator_loss_min\"]\n",
    "            self.generator_loss_max_min = study_info[\"generator_loss_max_min\"]\n",
    "            self.generator_loss_min_min = study_info[\"generator_loss_min_min\"]\n",
    "        else:\n",
    "            print(\"No info pkl file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_lr = 1e-3\n",
    "discriminator_lr = 1e-4\n",
    "batch_size = 1\n",
    "g_lr = generator_lr * batch_size\n",
    "d_lr = discriminator_lr * batch_size\n",
    "gan = Pix2PixSegmentation(generator_power=8, discriminator_power=8, generator_learning_rate=g_lr,discriminator_learning_rate=d_lr,\n",
    "                          test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_study_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_learning is True\n",
      "[Epoch 0/575] [Batch 0/6200] [D loss: 0.374644, acc:  65%] [G loss: 23.654783] time: 0:00:39.731345\n",
      "[Epoch 0/575] [Batch 3100/6200] [D loss: 0.009704, acc: 100%] [G loss: 12.051760] time: 0:10:38.099014\n",
      "discriminator_acces : 0.9641540822675151\n",
      "Mean generator_loss : 13.006329853919244\n",
      "Max generator_loss : 39.28605651855469\n",
      "Min generator_loss : 4.848196029663086\n",
      "generator loss decrease : 86.99367014608076\n",
      "generator loss decrease ratio : (0.13006329853919243)\n",
      "Max generator loss decrease : 960.7139434814453\n",
      "current lowest generator loss : 100\n",
      "current Learning_rate = 0.001\n",
      "save weights\n",
      "train_f1_loss : 9.072113037109375\n",
      "train_f1_score : 0.43299293518066406\n",
      "train_f1_rounded_score : 0.5368959903717041\n",
      "valid_f1_loss : 9.034845352172852\n",
      "valid_f1_score : 0.4353221654891968\n",
      "valid_f1_rounded_score : 0.5397965908050537\n",
      "discriminator_learning is False\n",
      "[Epoch 1/575] [Batch 0/6200] [D loss: 0.002755, acc:  99%] [G loss: 11.717185] time: 0:24:33.650681\n",
      "[Epoch 1/575] [Batch 3100/6200] [D loss: 0.001152, acc: 100%] [G loss: 12.360451] time: 0:32:11.763218\n",
      "discriminator_acces : 0.9996156852476058\n",
      "Mean generator_loss : 11.03953732894313\n",
      "Max generator_loss : 27.88465690612793\n",
      "Min generator_loss : 3.5653812885284424\n",
      "generator loss decrease : 1.966792524976114\n",
      "generator loss decrease ratio : (0.8487818971941994)\n",
      "Max generator loss decrease : 11.401399612426758\n",
      "current lowest generator loss : 13.006329853919244\n",
      "current Learning_rate = 0.0009984346464159642\n",
      "save weights\n",
      "train_f1_loss : 7.841150283813477\n",
      "train_f1_score : 0.5099281072616577\n",
      "train_f1_rounded_score : 0.5970516800880432\n",
      "valid_f1_loss : 7.873151779174805\n",
      "valid_f1_score : 0.5079280138015747\n",
      "valid_f1_rounded_score : 0.5879828929901123\n",
      "discriminator_learning is False\n",
      "[Epoch 2/575] [Batch 0/6200] [D loss: 0.000860, acc:  99%] [G loss: 10.146735] time: 0:44:02.543120\n",
      "[Epoch 2/575] [Batch 3100/6200] [D loss: 0.001107, acc: 100%] [G loss: 14.332288] time: 0:56:08.489289\n",
      "discriminator_acces : 0.9999210874495967\n",
      "Mean generator_loss : 10.419099113210555\n",
      "Max generator_loss : 29.5400447845459\n",
      "Min generator_loss : 2.772348403930664\n",
      "generator loss decrease : 0.620438215732575\n",
      "generator loss decrease ratio : (0.9437985309306461)\n",
      "Max generator loss decrease : -1.6553878784179688\n",
      "current lowest generator loss : 11.03953732894313\n",
      "current Learning_rate = 0.000996869020098343\n",
      "save weights\n",
      "train_f1_loss : 7.43401575088501\n",
      "train_f1_score : 0.5353740155696869\n",
      "train_f1_rounded_score : 0.6162620782852173\n",
      "valid_f1_loss : 7.547063827514648\n",
      "valid_f1_score : 0.5283085107803345\n",
      "valid_f1_rounded_score : 0.6027403473854065\n",
      "discriminator_learning is False\n",
      "[Epoch 3/575] [Batch 0/6200] [D loss: 0.000854, acc:  99%] [G loss: 9.624202] time: 1:12:26.377808\n",
      "[Epoch 3/575] [Batch 3100/6200] [D loss: 0.001165, acc: 100%] [G loss: 10.537112] time: 1:24:53.240416\n",
      "discriminator_acces : 0.9999122471963205\n",
      "Mean generator_loss : 9.925677614904219\n",
      "Max generator_loss : 33.88359451293945\n",
      "Min generator_loss : 2.545539140701294\n",
      "generator loss decrease : 0.4934214983063363\n",
      "generator loss decrease ratio : (0.952642594820821)\n",
      "Max generator loss decrease : -4.343549728393555\n",
      "current lowest generator loss : 10.419099113210555\n",
      "current Learning_rate = 0.0009953031205235184\n",
      "save weights\n",
      "train_f1_loss : 7.242332935333252\n",
      "train_f1_score : 0.5473541915416718\n",
      "train_f1_rounded_score : 0.6339145302772522\n",
      "valid_f1_loss : 7.408087730407715\n",
      "valid_f1_score : 0.5369945168495178\n",
      "valid_f1_rounded_score : 0.6212180852890015\n",
      "discriminator_learning is False\n",
      "[Epoch 4/575] [Batch 0/6200] [D loss: 0.000843, acc:  99%] [G loss: 8.354664] time: 1:41:00.915970\n",
      "[Epoch 4/575] [Batch 3100/6200] [D loss: 0.001184, acc: 100%] [G loss: 11.252401] time: 1:53:04.925633\n",
      "discriminator_acces : 0.9998916822864163\n",
      "Mean generator_loss : 9.64788195263955\n",
      "Max generator_loss : 34.58586883544922\n",
      "Min generator_loss : 2.246903657913208\n",
      "generator loss decrease : 0.2777956622646691\n",
      "generator loss decrease ratio : (0.9720124234291535)\n",
      "Max generator loss decrease : -0.7022743225097656\n",
      "current lowest generator loss : 9.925677614904219\n",
      "current Learning_rate = 0.000993736947165949\n",
      "save weights\n",
      "train_f1_loss : 7.335076332092285\n",
      "train_f1_score : 0.5415577292442322\n",
      "train_f1_rounded_score : 0.6256164312362671\n",
      "valid_f1_loss : 7.6330437660217285\n",
      "valid_f1_score : 0.522934764623642\n",
      "valid_f1_rounded_score : 0.5986768007278442\n",
      "discriminator_learning is False\n",
      "[Epoch 5/575] [Batch 0/6200] [D loss: 0.001215, acc:  99%] [G loss: 7.515430] time: 2:09:21.554009\n",
      "[Epoch 5/575] [Batch 3100/6200] [D loss: 0.001525, acc: 100%] [G loss: 7.416985] time: 2:21:26.433434\n"
     ]
    }
   ],
   "source": [
    "#gan.find_error = True\n",
    "#gan.find_error_epoch = 5\n",
    "\n",
    "#gan.start_epoch = 50\n",
    "gan.train(epochs=575, batch_size=batch_size, sample_interval=3100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54559326171875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gan.discriminator_acc_high_indexes_previous[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(3))\n",
    "\n",
    "print(a[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.test_on_batch([original_img, predicted_img], gan.valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.combined.train_on_batch(\n",
    "    [original_img, predicted_img],\n",
    "    [gan.valid_path, masked_img],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.loaded_data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gan.loaded_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.model_masked_img[:,:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.cvtColor(gan.model_masked_img[:,:,:,0][0], cv2.COLOR_GRAY2RGB).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.load_study_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gan.original_img.shape)\n",
    "print(gan.model_masked_img.shape)\n",
    "print(gan.masked_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_origin = ((gan.original_img[0]+1) * 127.5).astype('uint8')\n",
    "temp_masked = ((gan.masked_img[0]+1) * 127.5).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(temp_origin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gan.loaded_data[1][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "images = glob(\"C:\\\\Users\\\\gr300\\\\Desktop\\\\Works\\의료데이터\\\\Level_0_512_random_Split\\\\FOLD_1\\\\wo_SN\\\\slide-2020-04-22T09-53-31-R3-S20\\\\image_MONO_random\\\\*\")\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
