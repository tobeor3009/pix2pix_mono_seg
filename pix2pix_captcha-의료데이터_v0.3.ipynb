{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 75%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD 1 : logger 추가\n",
    "# TBD 2: flask github 참고, method, class, 파일의 맨 윗단 마다 pydoc 형식으로 달기\n",
    "# TBD 3: 축약어를 자제할것 (특히 변수)\n",
    "\n",
    "# tensorflow Module\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as keras_backend\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "# python basic Module\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "from datetime import datetime\n",
    "from shutil import copy\n",
    "from pickle import dump, load\n",
    "\n",
    "# math, image, plot Module\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt  # TBD\n",
    "\n",
    "# email Module\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.header import Header\n",
    "\n",
    "from data_loader.medical_segmentation_data_loader_v1 import DataLoader\n",
    "\n",
    "from gan_module.model import build_generator, build_discriminator\n",
    "from gan_module.draw_images import ImageDrawer\n",
    "from gan_module.custom_loss import f1_loss_for_training, f1_score, dice_loss_for_training\n",
    "\n",
    "# set GPU memory growth allocation\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\"\"\"\n",
    "if you don't have nvidia-gpu, try plaidml! but it will works tensorflow 1.x.x\n",
    "# pip install -U plaidml-keras\n",
    "# plaidml-setup\n",
    "\"\"\"\n",
    "# use_plaidml = False\n",
    "# if use_plaidml :\n",
    "#     import plaidml.keras\n",
    "#     plaidml.keras.install_backend()\n",
    "#     os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "# class CaptchaDiscriminator\n",
    "# captcha_generator = CaptchaGenerator()\n",
    "# CaptchaGeneratorTrainer(captcha_generator, discriminator)\n",
    "# class CaptchaGeneratorTrainer():\n",
    "# class LearningRate():\n",
    "# class Smtp()\n",
    "# class Shape()\n",
    "# class InputShape(Shape):\n",
    "# class OutputShape(Shape):\n",
    "# class Model():\n",
    "#     def save():\n",
    "#     def load():\n",
    "# class Generator(Model) :\n",
    "#     @override\n",
    "#     def save():\n",
    "#         super()\n",
    "# class HyperParameter()\n",
    "\n",
    "\n",
    "class Pix2PixSegmentation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_power=32,\n",
    "        discriminator_power=32,\n",
    "        generator_learning_rate=1e-4,\n",
    "        discriminator_learning_rate=1e-4,\n",
    "        learning_rate_decay_rate_epoch=0.1,\n",
    "        learning_rate_decay_rate_dynamic=0.1,\n",
    "        find_init_epoch=150,\n",
    "        find_error=False,\n",
    "        temp_weights_path=\".\",\n",
    "        draw_images=True,\n",
    "        on_memory=True,\n",
    "        test=False\n",
    "    ):\n",
    "        # smtp info\n",
    "        self.smtp_host = \"smtp.gmail.com\"\n",
    "        self.smtp_port = 465\n",
    "        self.smtp_id = \"rpa.manager0001@gmail.com\"\n",
    "        self.smtp_password = \"!rpa.admin!23\"\n",
    "        self.smtp_to_addr = \"tobeor3009@gmail.com\"\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 512\n",
    "        self.img_cols = 512\n",
    "        self.input_channels = 3\n",
    "        self.output_channels = 1\n",
    "        self.input_img_shape = (\n",
    "            self.img_rows, self.img_cols, self.input_channels)\n",
    "        self.output_img_shape = (\n",
    "            self.img_rows, self.img_cols, self.output_channels)\n",
    "        # set parameter\n",
    "        self.start_epoch = None\n",
    "        self.history = [[], [], []]\n",
    "        self.f1_loss_ratio = 25\n",
    "        self.huber_loss_ratio = 100\n",
    "        self.learning_rate_decay_rate_epoch = learning_rate_decay_rate_epoch\n",
    "        self.learning_rate_decay_rate_dynamic = learning_rate_decay_rate_dynamic\n",
    "        self.find_init_epoch = find_init_epoch\n",
    "        self.find_error = find_error\n",
    "        self.find_error_epoch = 30\n",
    "        self.error_list = []\n",
    "        self.temp_weights_path = temp_weights_path\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = \"tumor\"\n",
    "        self.data_loader = DataLoader(\n",
    "            dataset_name=self.dataset_name,\n",
    "            img_res=(self.img_rows, self.img_cols),\n",
    "            on_memory=on_memory, test=test\n",
    "        )\n",
    "        self.train_loaded_data, self.valid_loaded_data = self.data_loader.load_all()\n",
    "        if test:\n",
    "            self.train_loaded_data_len = 20\n",
    "            self.valid_loaded_data_len = 20\n",
    "        else:\n",
    "            self.train_loaded_data_len = self.data_loader.train_data_length\n",
    "            self.valid_loaded_data_len = self.data_loader.valid_data_length\n",
    "\n",
    "        self.train_loaded_data_index = np.arange(self.train_loaded_data_len)\n",
    "        self.valid_loaded_data_index = np.arange(self.valid_loaded_data_len)\n",
    "\n",
    "        # Configure Image Drawer\n",
    "        self.draw_images = draw_images\n",
    "        self.image_drawer = ImageDrawer(\n",
    "            dataset_name=self.dataset_name, data_loader=self.data_loader\n",
    "        )\n",
    "        # training parameters\n",
    "        self.learning_schedule = [\n",
    "            50,\n",
    "            100,\n",
    "            150,\n",
    "            200,\n",
    "            250,\n",
    "            300,\n",
    "            350,\n",
    "            400,\n",
    "            450,\n",
    "            500,\n",
    "            550\n",
    "        ]\n",
    "        self.discriminator_acc_previous = 0.5\n",
    "        self.discriminator_loss_high_indexes = [i for i in range(train_loaded_data_len)]\n",
    "        self.discriminator_loss_high_indexes_previous = [i for i in range(train_loaded_data_len)]\n",
    "        self.generator_loss_min = 100\n",
    "        self.generator_loss_previous = 100\n",
    "        self.generator_loss_max_previous = 1000\n",
    "        self.generator_loss_max_min = 1000\n",
    "        self.generator_loss_min_min = 1000\n",
    "        self.weight_save_stack = False\n",
    "        self.weight_stagnant_stack = 0\n",
    "        self.look_on_for_generator_min_loss = False\n",
    "        self.learning_rate_decay_dynamic = 0\n",
    "        self.training_end_stack = 0\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2 ** 2)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.generator_power = generator_power\n",
    "        self.discriminator_power = discriminator_power\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "        generator_optimizer = Nadam(self.generator_learning_rate)\n",
    "        discriminator_optimizer = Nadam(self.discriminator_learning_rate)\n",
    "\n",
    "        # layer Component\n",
    "        self.kernel_initializer = RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator(\n",
    "            input_img_shape=self.input_img_shape,\n",
    "            output_img_shape=self.output_img_shape,\n",
    "            discriminator_power=self.discriminator_power,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "        )\n",
    "        # self.discriminator = self.build_discriminator()\n",
    "        # 'mse' or tf.keras.losses.Huber() tf.keras.losses.LogCosh()\n",
    "        self.discriminator.compile(\n",
    "            loss=tf.keras.losses.LogCosh(),\n",
    "            optimizer=discriminator_optimizer,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        # -------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = build_generator(\n",
    "            input_img_shape=self.input_img_shape,\n",
    "            output_channels=self.output_channels,\n",
    "            generator_power=self.generator_power,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "        )\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        original_img = Input(shape=self.input_img_shape)\n",
    "        masked_img = Input(shape=self.output_img_shape)\n",
    "        # generate image from original_img for target masked_img\n",
    "        model_masked_img = self.generator(original_img)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        model_validity = self.discriminator([original_img,model_masked_img])\n",
    "        # give score by\n",
    "        # 1. how generator trick discriminator\n",
    "        # 2. how generator's image same as real photo in pixel\n",
    "        # 3. if you want change loss, see doc https://keras.io/api/losses/\n",
    "        # 4. 'mse', 'mae', tf.keras.losses.LogCosh(),  tf.keras.losses.Huber()\n",
    "        self.combined = Model(\n",
    "            inputs=[original_img, masked_img],\n",
    "            outputs=[model_validity, model_masked_img, model_masked_img],\n",
    "        )\n",
    "        self.combined.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.LogCosh(),\n",
    "                tf.keras.losses.Huber(),\n",
    "                dice_loss_for_training\n",
    "            ],\n",
    "            loss_weights=[0.5, self.huber_loss_ratio, self.f1_loss_ratio],\n",
    "            optimizer=generator_optimizer\n",
    "        )\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        self.training_end_stack = 0\n",
    "        self.batch_size = batch_size\n",
    "        valid_patch = np.ones((self.batch_size,) +\n",
    "                              self.disc_patch, dtype=np.float32)\n",
    "        fake_patch = np.zeros((self.batch_size,) +\n",
    "                              self.disc_patch, dtype=np.float32)\n",
    "        if self.start_epoch is None:\n",
    "            self.start_epoch = 0\n",
    "        for epoch in range(self.start_epoch, epochs):\n",
    "            batch_i = 0\n",
    "            discriminator_acces = []\n",
    "            train_generator_loss = []\n",
    "            generator_loss_max_in_epoch = 0\n",
    "            generator_loss_min_in_epoch = 1000\n",
    "            generator_current_learning_rate = self.learning_rate_scheduler(\n",
    "                self.generator_learning_rate\n",
    "                * (2 ** (self.learning_rate_decay_dynamic)),\n",
    "                epoch,\n",
    "            )\n",
    "            discriminator_current_learning_rate = self.learning_rate_scheduler(\n",
    "                self.discriminator_learning_rate\n",
    "                * (2 ** (self.learning_rate_decay_dynamic)),\n",
    "                epoch,\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.combined.optimizer.learning_rate, generator_current_learning_rate\n",
    "            )\n",
    "            keras_backend.set_value(\n",
    "                self.discriminator.optimizer.learning_rate,\n",
    "                discriminator_current_learning_rate,\n",
    "            )\n",
    "            # shffle data 10 epoch term\n",
    "            if epoch % 10 == 0 and not isinstance(self.train_loaded_data, types.GeneratorType):\n",
    "                np.random.shuffle(self.train_loaded_data_index)\n",
    "                self.train_loaded_data = [\n",
    "                    self.train_loaded_data[i][self.train_loaded_data_index] for i in range(2)\n",
    "                ]\n",
    "            if self.discriminator_acc_previous < 0.75:\n",
    "                discriminator_learning = True\n",
    "                print(\"discriminator_learning is True\")\n",
    "            else:\n",
    "                discriminator_learning = False\n",
    "                print(\"discriminator_learning is False\")\n",
    "                \n",
    "            while batch_i + self.batch_size <= self.train_loaded_data_len:\n",
    "                original_img = self.train_loaded_data[0][batch_i: batch_i +\n",
    "                                                         self.batch_size]\n",
    "                masked_img = self.train_loaded_data[1][batch_i: batch_i +\n",
    "                                                       self.batch_size]\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                # Condition on B and generate a translated version\n",
    "                model_masked_img = self.generator.predict_on_batch(\n",
    "                    original_img)\n",
    "\n",
    "                # forTest\n",
    "                self.masked_img = masked_img\n",
    "                self.original_img = original_img\n",
    "                self.model_masked_img = model_masked_img\n",
    "                self.valid_path = valid_patch\n",
    "                self.fake_patch = fake_patch\n",
    "                # Train the discriminators (target image = masked_img / generated_img = model_masked_img)\n",
    "                self.discriminator.train_on_batch([original_img, masked_img], valid_patch)\n",
    "\n",
    "                \n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                generator_loss = self.combined.train_on_batch(\n",
    "                    [original_img, masked_img],\n",
    "                    [valid_patch, masked_img, masked_img],\n",
    "                )\n",
    "                if discriminator_learning:\n",
    "                    discriminator_loss = self.discriminator.train_on_batch([original_img, model_masked_img], fake_patch)\n",
    "                else:\n",
    "                    discriminator_loss = self.discriminator.test_on_batch([original_img, model_masked_img], fake_patch)                \n",
    "                elapsed_time = datetime.now() - start_time\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    # Plot the progress\n",
    "                    print(\n",
    "                        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\"\n",
    "                        % (\n",
    "                            epoch,\n",
    "                            epochs,\n",
    "                            batch_i,\n",
    "                            self.train_loaded_data_len,\n",
    "                            discriminator_loss[0],\n",
    "                            100 * discriminator_loss[1],\n",
    "                            generator_loss[0],\n",
    "                            elapsed_time,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0 and self.draw_images:\n",
    "                    self.image_drawer.sample_images(\n",
    "                        self.generator, epoch, batch_i)\n",
    "\n",
    "                discriminator_acces.append(discriminator_loss[1])\n",
    "                train_generator_loss.append(generator_loss[0])\n",
    "                # loss 가 가장 높은 이미지를 저장 및 max_in_epoch 갱신\n",
    "                if generator_loss[0] > generator_loss_max_in_epoch:\n",
    "                    model_masked_img = self.generator.predict_on_batch(\n",
    "                        original_img)\n",
    "                    if self.draw_images:\n",
    "                        self.image_drawer.draw_worst_and_best(\n",
    "                            original_img,\n",
    "                            model_masked_img,\n",
    "                            masked_img,\n",
    "                            epoch,\n",
    "                            worst=True,\n",
    "                        )\n",
    "                    generator_loss_max_in_epoch = generator_loss[0]\n",
    "                # loss 가 가장 낮은 이미지를 저장 및 max_in_epoch 갱신\n",
    "                if generator_loss_min_in_epoch > generator_loss[0]:\n",
    "                    model_masked_img = self.generator.predict_on_batch(\n",
    "                        original_img)\n",
    "                    if self.draw_images:\n",
    "                        self.image_drawer.draw_worst_and_best(\n",
    "                            original_img,\n",
    "                            model_masked_img,\n",
    "                            masked_img,\n",
    "                            epoch,\n",
    "                            worst=False,\n",
    "                        )\n",
    "                    generator_loss_min_in_epoch = generator_loss[0]\n",
    "\n",
    "                # 한 배치 끝\n",
    "                batch_i += self.batch_size\n",
    "            # training batch 사이클 끝\n",
    "            self.history[0].append(discriminator_loss[0])\n",
    "            self.history[1].append(generator_loss[0])\n",
    "            self.history[2].append(100 * discriminator_loss[1])\n",
    "            print(f\"discriminator_acces : {str(np.mean(discriminator_acces))}\")\n",
    "            print(\n",
    "                f\"Mean generator_loss : {str(np.mean(train_generator_loss))}\")\n",
    "            print(f\"Max generator_loss : {str(np.max(train_generator_loss))}\")\n",
    "            print(f\"Min generator_loss : {str(np.min(train_generator_loss))}\")\n",
    "            print(\n",
    "                f\"generator loss decrease : {str(self.generator_loss_previous - np.mean(train_generator_loss))}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Max generator loss decrease : {str(self.generator_loss_max_previous - np.max(train_generator_loss))}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"current lowest generator loss : {str(self.generator_loss_min)}\")\n",
    "\n",
    "            # set look_on_for_generator_min_loss property if min loss decrease:\n",
    "            if (\n",
    "                self.generator_loss_min_min > generator_loss_min_in_epoch\n",
    "                and self.generator_loss_max_min * 2 > generator_loss_max_in_epoch\n",
    "            ):\n",
    "                self.look_on_for_generator_min_loss = True\n",
    "            else:\n",
    "                self.look_on_for_generator_min_loss = False\n",
    "            # rollback if loss not converge\n",
    "            if np.mean(train_generator_loss) / self.generator_loss_min < 1.02:\n",
    "                if self.generator_loss_min > np.mean(train_generator_loss):\n",
    "                    # 학습중일때 진전이 너무 더디다면 learning_rate 을 조정하는 스택을 추가.\n",
    "                    # loss의 감소량이 너무 적을때\n",
    "                    if np.mean(train_generator_loss) / self.generator_loss_min > 0.97:\n",
    "                        self.learning_rate_decay_dynamic -= (\n",
    "                            self.learning_rate_decay_rate_dynamic / 10\n",
    "                        )\n",
    "                        print(\n",
    "                            \"increase Learning rate(learning_rate_increaseStack =\"\n",
    "                            + str(round(self.learning_rate_decay_dynamic, 3))\n",
    "                            + \")\"\n",
    "                            + \"ratio = (\"\n",
    "                            + str(np.mean(train_generator_loss) /\n",
    "                                  self.generator_loss_min)\n",
    "                            + \")\"\n",
    "                        )\n",
    "                    elif np.mean(train_generator_loss) / self.generator_loss_min < 0.85:\n",
    "                        # loss의 감소량이 너무 클때\n",
    "                        self.learning_rate_decay_dynamic -= (\n",
    "                            self.learning_rate_decay_rate_dynamic / 10\n",
    "                        )\n",
    "                        print(\n",
    "                            \"decrease Learning rate(learning_rate_increaseStack =\"\n",
    "                            + str(round(self.learning_rate_decay_dynamic, 3))\n",
    "                            + \")\"\n",
    "                            + \"ratio = (\"\n",
    "                            + str(np.mean(train_generator_loss) /\n",
    "                                  self.generator_loss_min)\n",
    "                            + \")\"\n",
    "                        )\n",
    "                    self.generator_loss_min = np.mean(train_generator_loss)\n",
    "                    self.generator_loss_max_min = generator_loss_max_in_epoch\n",
    "                    self.generator_loss_min_min = generator_loss_min_in_epoch\n",
    "                    self.weight_save_stack = True\n",
    "                    self.save_study_info()\n",
    "                    print(\"save weights\")\n",
    "                else:\n",
    "                    # loss가 약간 증가 했을때 (정체 가능성)\n",
    "                    self.learning_rate_decay_dynamic -= (\n",
    "                        self.learning_rate_decay_rate_dynamic / 2\n",
    "                    )\n",
    "                    print(\n",
    "                        \"decrease Learning rate(learning_rate_increaseStack =\"\n",
    "                        + str(round(self.learning_rate_decay_dynamic, 3))\n",
    "                        + \")\"\n",
    "                        + \"ratio = (\"\n",
    "                        + str(np.mean(train_generator_loss) /\n",
    "                              self.generator_loss_min)\n",
    "                        + \")\"\n",
    "                    )\n",
    "            else:\n",
    "                if self.look_on_for_generator_min_loss:\n",
    "                    print(\"min_loss is decreased. watch\")\n",
    "                else:\n",
    "                    print(\"loss decreasing\")\n",
    "                    self.learning_rate_decay_dynamic -= (\n",
    "                        self.learning_rate_decay_rate_dynamic\n",
    "                    )\n",
    "                    print(\n",
    "                        \"decrease Learning rate(learning_rate_increaseStack =\"\n",
    "                        + str(round(self.learning_rate_decay_dynamic, 3))\n",
    "                        + \")\"\n",
    "                    )\n",
    "                    self.load_best_weights()\n",
    "            # set look_on_for_generator_min_loss property False anyway because of confusing training\n",
    "            self.look_on_for_generator_min_loss = False\n",
    "            # previous generator_loss 갱신\n",
    "            self.generator_loss_previous = np.mean(train_generator_loss)\n",
    "            self.generator_loss_max_previous = generator_loss_max_in_epoch\n",
    "\n",
    "            if epoch >= 10 and self.weight_save_stack:\n",
    "                copy(\n",
    "                    \"generator.h5\",\n",
    "                    \"./generator_weights/generator_\"\n",
    "                    + str(round(self.generator_loss_min, 5))\n",
    "                    + \"_\"\n",
    "                    + str(round(self.generator_loss_max_min, 5))\n",
    "                    + \".h5\",\n",
    "                )\n",
    "                self.weight_save_stack = False\n",
    "\n",
    "            self.discriminator_acc_previous = np.mean(discriminator_acces)\n",
    "\n",
    "            train_f1_loss_list = []\n",
    "            train_f1_score_list = []\n",
    "            train_predict_mini_batch_size = 1\n",
    "            for index in range(0, self.train_loaded_data_len, train_predict_mini_batch_size):\n",
    "                \n",
    "                train_original_img = self.train_loaded_data[0][index:index+train_predict_mini_batch_size]\n",
    "                train_masked_img = self.train_loaded_data[1][index:index+train_predict_mini_batch_size]\n",
    "                train_model_masked_img = self.generator.predict_on_batch(\n",
    "                self.train_loaded_data[0][index:index+train_predict_mini_batch_size])\n",
    "                \n",
    "                train_f1_loss = f1_loss_for_training(train_masked_img, np.squeeze(train_model_masked_img))\n",
    "                train_f1_score = f1_score(train_masked_img, np.squeeze(train_model_masked_img))\n",
    "                train_f1_loss_list.append(train_f1_loss)\n",
    "                train_f1_score_list.append(train_f1_score)\n",
    "            print(f\"train_f1_loss : {np.mean(train_f1_loss_list)* self.f1_loss_ratio}\")\n",
    "            print(f\"train_f1_score : {np.mean(train_f1_score_list)}\")\n",
    "            \n",
    "            valid_f1_loss_list = []\n",
    "            valid_f1_score_list = []\n",
    "            valid_predict_mini_batch_size = 1\n",
    "            for index in range(0, self.valid_loaded_data_len, valid_predict_mini_batch_size):\n",
    "                \n",
    "                valid_original_img = self.valid_loaded_data[0][index:index+valid_predict_mini_batch_size]\n",
    "                valid_masked_img = self.valid_loaded_data[1][index:index+valid_predict_mini_batch_size]\n",
    "                valid_model_masked_img = self.generator.predict_on_batch(\n",
    "                self.valid_loaded_data[0][index:index+valid_predict_mini_batch_size])\n",
    "                \n",
    "                valid_f1_loss = f1_loss_for_training(valid_masked_img, np.squeeze(valid_model_masked_img))\n",
    "                valid_f1_score = f1_score(valid_masked_img, np.squeeze(valid_model_masked_img))\n",
    "                valid_f1_loss_list.append(valid_f1_loss)\n",
    "                valid_f1_score_list.append(valid_f1_score)\n",
    "            print(f\"valid_f1_loss : {np.mean(valid_f1_loss_list)* self.f1_loss_ratio}\")\n",
    "            print(f\"valid_f1_score : {np.mean(valid_f1_score_list)}\")\n",
    "    def learning_rate_scheduler(self, learning_rate, epoch):\n",
    "\n",
    "        for step in range(0, len(self.learning_schedule)):\n",
    "            if epoch < self.learning_schedule[step]:\n",
    "                break\n",
    "        new_learning_rate = learning_rate * (\n",
    "            self.learning_rate_decay_rate_epoch ** (step)\n",
    "        )\n",
    "        return new_learning_rate\n",
    "\n",
    "    def get_info_folderPath(self):\n",
    "        return (\n",
    "            str(round(self.generator_loss_min, 5))\n",
    "            + \"_\"\n",
    "            + str(round(self.generator_loss_max_min, 5))\n",
    "            + \"_\"\n",
    "            + str(round(self.learning_rate_decay_dynamic, 3))\n",
    "        )\n",
    "\n",
    "    def save_study_info(self, path=None):\n",
    "\n",
    "        if path == None:\n",
    "            path = self.temp_weights_path\n",
    "\n",
    "        generator_weigth_path = os.path.join(path, \"generator.h5\")\n",
    "        discriminator_weigth_path = os.path.join(path, \"discriminator.h5\")\n",
    "        combined_weigth_path = os.path.join(path, \"combined.h5\")\n",
    "\n",
    "        self.generator.save_weights(generator_weigth_path)\n",
    "        self.discriminator.save_weights(discriminator_weigth_path)\n",
    "        self.combined.save_weights(combined_weigth_path)\n",
    "\n",
    "        study_info = {}\n",
    "        study_info[\"start_epoch\"] = self.start_epoch\n",
    "        study_info[\"generator_loss_min\"] = self.generator_loss_min\n",
    "        study_info[\"generator_loss_max_min\"] = self.generator_loss_max_min\n",
    "        study_info[\"generator_loss_min_min\"] = self.generator_loss_min_min\n",
    "        study_info[\"learning_rate_decay_dynamic\"] = self.learning_rate_decay_dynamic\n",
    "\n",
    "        file = open(path + \"/study_info.pkl\", \"wb\")\n",
    "        dump(study_info, file)\n",
    "        file.close()\n",
    "\n",
    "    def load_best_weights(self):\n",
    "        self.generator.load_weights(self.temp_weights_path + \"/generator.h5\")\n",
    "        self.discriminator.load_weights(\n",
    "            self.temp_weights_path + \"/discriminator.h5\")\n",
    "        self.combined.load_weights(self.temp_weights_path + \"/combined.h5\")\n",
    "\n",
    "    def load_study_info(self):\n",
    "\n",
    "        self.generator.load_weights(\"generator.h5\")\n",
    "        self.discriminator.load_weights(\"discriminator.h5\")\n",
    "        self.combined.load_weights(\"combined.h5\")\n",
    "\n",
    "        if os.path.isfile(\"study_info.pkl\"):\n",
    "            file = open(\"study_info.pkl\", \"rb\")\n",
    "            study_info = load(file)\n",
    "            file.close()\n",
    "            self.start_epoch = study_info[\"start_epoch\"]\n",
    "            self.generator_loss_min = study_info[\"generator_loss_min\"]\n",
    "            self.generator_loss_max_min = study_info[\"generator_loss_max_min\"]\n",
    "            self.generator_loss_min_min = study_info[\"generator_loss_min_min\"]\n",
    "            self.learning_rate_decay_dynamic = study_info[\"learning_rate_decay_dynamic\"]\n",
    "        else:\n",
    "            print(\"No info pkl file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = Pix2PixSegmentation(generator_power=4, discriminator_power=4, generator_learning_rate=1e-4,discriminator_learning_rate=1e-4,\n",
    "                          learning_rate_decay_rate_epoch = 0.5, learning_rate_decay_rate_dynamic = 0.1, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.load_study_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_learning is True\n",
      "[Epoch 50/575] [Batch 0/6200] [D loss: 0.223090, acc:   1%] [G loss: 5.790051] time: 0:00:05.910429\n",
      "[Epoch 50/575] [Batch 3400/6200] [D loss: 0.000672, acc: 100%] [G loss: 10.267649] time: 0:07:18.485498\n",
      "discriminator_acces : 0.9956481244487148\n",
      "Mean generator_loss : 8.391539911185541\n",
      "Max generator_loss : 41.698081970214844\n",
      "Min generator_loss : 1.0599838495254517\n",
      "generator loss decrease : 91.60846008881445\n",
      "Max generator loss decrease : 958.3019180297852\n",
      "current lowest generator loss : 8.277445065523347\n",
      "decrease Learning rate(learning_rate_increaseStack =-1.23)ratio = (1.0137838239648866)\n",
      "train_f1_loss : 6.177092343568802\n",
      "train_f1_score : 0.7588476538658142\n",
      "valid_f1_loss : 8.595120161771774\n",
      "valid_f1_score : 0.6615681052207947\n",
      "discriminator_learning is False\n",
      "[Epoch 51/575] [Batch 0/6200] [D loss: 0.008017, acc:  98%] [G loss: 10.594647] time: 0:16:17.107444\n",
      "[Epoch 51/575] [Batch 3400/6200] [D loss: 0.027430, acc:  99%] [G loss: 10.197916] time: 0:23:04.146978\n",
      "discriminator_acces : 0.6972524827526462\n",
      "Mean generator_loss : 8.221187006869624\n",
      "Max generator_loss : 41.773624420166016\n",
      "Min generator_loss : 0.9629009366035461\n",
      "generator loss decrease : 0.17035290431591754\n",
      "Max generator loss decrease : -0.07554244995117188\n",
      "current lowest generator loss : 8.277445065523347\n",
      "increase Learning rate(learning_rate_increaseStack =-1.24)ratio = (0.9932034512813567)\n",
      "save weights\n",
      "train_f1_loss : 6.191584840416908\n",
      "train_f1_score : 0.758283793926239\n",
      "valid_f1_loss : 8.635083585977554\n",
      "valid_f1_score : 0.6601657867431641\n",
      "discriminator_learning is True\n",
      "[Epoch 52/575] [Batch 0/6200] [D loss: 0.255430, acc:   2%] [G loss: 12.807767] time: 0:31:39.365810\n",
      "[Epoch 52/575] [Batch 3400/6200] [D loss: 0.000694, acc: 100%] [G loss: 10.523022] time: 0:38:54.415508\n",
      "discriminator_acces : 0.996414086126512\n",
      "Mean generator_loss : 8.282835117655416\n",
      "Max generator_loss : 41.76045227050781\n",
      "Min generator_loss : 1.095859169960022\n",
      "generator loss decrease : -0.06164811078579291\n",
      "Max generator loss decrease : 0.013172149658203125\n",
      "current lowest generator loss : 8.221187006869624\n",
      "decrease Learning rate(learning_rate_increaseStack =-1.29)ratio = (1.0074986873226797)\n",
      "train_f1_loss : 6.18814118206501\n",
      "train_f1_score : 0.7583803534507751\n",
      "valid_f1_loss : 8.69971588253975\n",
      "valid_f1_score : 0.6574302911758423\n",
      "discriminator_learning is False\n",
      "[Epoch 53/575] [Batch 0/6200] [D loss: 0.001270, acc:  99%] [G loss: 10.479290] time: 0:47:45.838912\n",
      "[Epoch 53/575] [Batch 3400/6200] [D loss: 0.033158, acc:  99%] [G loss: 10.080847] time: 0:54:18.381063\n",
      "discriminator_acces : 0.7134185594128024\n",
      "Mean generator_loss : 8.15947528314206\n",
      "Max generator_loss : 41.81486892700195\n",
      "Min generator_loss : 0.8972498774528503\n",
      "generator loss decrease : 0.12335983451335686\n",
      "Max generator loss decrease : -0.054416656494140625\n",
      "current lowest generator loss : 8.221187006869624\n",
      "increase Learning rate(learning_rate_increaseStack =-1.3)ratio = (0.9924935749939762)\n",
      "save weights\n",
      "train_f1_loss : 6.161385402083397\n",
      "train_f1_score : 0.7595396041870117\n",
      "valid_f1_loss : 8.653980493545532\n",
      "valid_f1_score : 0.659415066242218\n",
      "discriminator_learning is True\n",
      "[Epoch 54/575] [Batch 0/6200] [D loss: 0.305837, acc:   0%] [G loss: 9.848190] time: 1:02:36.132159\n",
      "[Epoch 54/575] [Batch 3400/6200] [D loss: 0.000643, acc: 100%] [G loss: 10.206055] time: 1:09:49.128525\n",
      "discriminator_acces : 0.9960679380355343\n",
      "Mean generator_loss : 8.216825974900877\n",
      "Max generator_loss : 41.24600601196289\n",
      "Min generator_loss : 0.8162739276885986\n",
      "generator loss decrease : -0.05735069175881691\n",
      "Max generator loss decrease : 0.5688629150390625\n",
      "current lowest generator loss : 8.15947528314206\n",
      "decrease Learning rate(learning_rate_increaseStack =-1.35)ratio = (1.007028723020622)\n",
      "train_f1_loss : 6.241098046302795\n",
      "train_f1_score : 0.7564491629600525\n",
      "valid_f1_loss : 8.730807900428772\n",
      "valid_f1_score : 0.6564016342163086\n",
      "discriminator_learning is False\n",
      "[Epoch 55/575] [Batch 0/6200] [D loss: 0.001340, acc: 100%] [G loss: 8.933983] time: 1:18:37.112585\n"
     ]
    }
   ],
   "source": [
    "#gan.find_error = True\n",
    "#gan.find_error_epoch = 5\n",
    "\n",
    "gan.start_epoch = 50\n",
    "gan.train(epochs=575, batch_size=1, sample_interval=3400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(3))\n",
    "\n",
    "print(a[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.test_on_batch([original_img, predicted_img], gan.valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.combined.train_on_batch(\n",
    "    [original_img, predicted_img],\n",
    "    [gan.valid_path, masked_img],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.loaded_data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gan.loaded_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.model_masked_img[:,:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.cvtColor(gan.model_masked_img[:,:,:,0][0], cv2.COLOR_GRAY2RGB).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.load_study_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gan.original_img.shape)\n",
    "print(gan.model_masked_img.shape)\n",
    "print(gan.masked_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_origin = ((gan.original_img[0]+1) * 127.5).astype('uint8')\n",
    "temp_masked = ((gan.masked_img[0]+1) * 127.5).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(temp_origin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gan.loaded_data[1][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "images = glob(\"C:\\\\Users\\\\gr300\\\\Desktop\\\\Works\\의료데이터\\\\Level_0_512_random_Split\\\\FOLD_1\\\\wo_SN\\\\slide-2020-04-22T09-53-31-R3-S20\\\\image_MONO_random\\\\*\")\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
